import os
project_path=os.getcwd()


project_path=join(project_path,"google trends\\google trends")
os.path.join(project_path)


df = pd.read_csv(os.path.join(project_path,"AL_mosquito.csv"))


df.head()


import pandas as pd
import os
# import pycountry

# Define a mapping for ISO 3166-2:BR state codes to state names
state_code_to_name = {
    "AC": "State of Acre", "AL": "State of Alagoas", "AP": "State of Amapá",
    "AM": "State of Amazonas", "BA": "State of Bahia", "CE": "State of Ceará",
    "DF": "Federal District", "ES": "State of Espírito Santo", "GO": "State of Goiás",
    "MA": "State of Maranhão", "MT": "State of Mato Grosso", "MS": "State of Mato Grosso do Sul",
    "MG": "State of Minas Gerais", "PA": "State of Pará", "PB": "State of Paraíba",
    "PR": "State of Paraná", "PE": "State of Pernambuco", "PI": "State of Piauí",
    "RJ": "State of Rio de Janeiro", "RN": "State of Rio Grande do Norte",
    "RS": "State of Rio Grande do Sul", "RO": "State of Rondônia", "RR": "State of Roraima",
    "SC": "State of Santa Catarina", "SP": "State of São Paulo", "SE": "State of Sergipe",
    "TO": "State of Tocantins"
}
state_code_to_name2 = {
    "AC": "Acre", "AL": "Alagoas", "AP": "Amapá",
    "AM": "Amazonas", "BA": "Bahia", "CE": "Ceará",
    "DF": "Federal District", "ES": "Espírito Santo", "GO": "Goiás",
    "MA": "Maranhão", "MT": "Mato Grosso", "MS": "Mato Grosso do Sul",
    "MG": "Minas Gerais", "PA": "Pará", "PB": "Paraíba",
    "PR": "Paraná", "PE": "Pernambuco", "PI": "Piauí",
    "RJ": "Rio de Janeiro", "RN": "Rio Grande do Norte",
    "RS": "Rio Grande do Sul", "RO": "Rondônia", "RR": "Roraima",
    "SC": "Santa Catarina", "SP": "São Paulo", "SE": "Sergipe",
    "TO": "Tocantins"
}
# # Define the project path
# project_path = os.getcwd()
# project_path = os.path.join(project_path, "google trends", "google trends")

# # Load the CSV file
# # file_name = "AL_sintomas_dengue.csv"
# file_name = "AL_mosquito.csv"
# df = pd.read_csv(os.path.join(project_path, file_name), index_col=0)

# # Convert the index to a column
# df.reset_index(inplace=True)

# # Extract the state code and category from the file name
# file_state_code, *file_category = file_name.split('.')[0].split('_')
# file_category = ' '.join(file_category)

# # Extract the corresponding state name and category from the first entry
# first_entry = df.iloc[0, 1]  # Get the first value in the "Category: All categories" column
# entry_category, entry_state = first_entry.split(':', 1)
# entry_state = entry_state.strip(" ()")  # Remove parentheses and whitespace

# # Verify the match
# is_state_match = state_code_to_name[file_state_code] == entry_state or state_code_to_name2[file_state_code] == entry_state
# is_category_match = file_category.lower() == entry_category.strip().lower()

# # Output results
# print("File Name:", file_name)
# print("First Entry in 'Category: All categories':", first_entry)
# print("State Match:", is_state_match)
# print("Category Match:", is_category_match)
# print("Overall Match:", is_state_match and is_category_match)

# # Convert the first row to column names
# df.columns = df.iloc[0]
# df = df[1:]  # Drop the first row as it's now used as the header

# entry_category=entry_category.split(' ')
# entry_category='_'.join(entry_category)
# entry_state=entry_state.split(' ')
# entry_state='_'.join(entry_state)


# # Rename the second column by appending "score"
# df.columns = [df.columns[0]] + [f"{entry_category} {entry_state} score"]

# column_name = df.columns[1]  # Get the name of the second column
# df[column_name] = pd.to_numeric(df[column_name], errors='coerce').astype('Int64')

# # # Display the updated DataFrame
# df.head()


print(len(state_code_to_name))


import os
import pandas as pd

# Define mappings for state codes to state names
state_code_to_name = {
    "AC": "State of Acre", "AL": "State of Alagoas", "AP": "State of Amapá",
    "AM": "State of Amazonas", "BA": "State of Bahia", "CE": "State of Ceará",
    "DF": "Federal District", "ES": "State of Espírito Santo", "GO": "State of Goiás",
    "MA": "State of Maranhão", "MT": "State of Mato Grosso", "MS": "State of Mato Grosso do Sul",
    "MG": "State of Minas Gerais", "PA": "State of Pará", "PB": "State of Paraíba",
    "PR": "State of Paraná", "PE": "State of Pernambuco", "PI": "State of Piauí",
    "RJ": "State of Rio de Janeiro", "RN": "State of Rio Grande do Norte",
    "RS": "State of Rio Grande do Sul", "RO": "State of Rondônia", "RR": "State of Roraima",
    "SC": "State of Santa Catarina", "SP": "State of São Paulo", "SE": "State of Sergipe",
    "TO": "State of Tocantins"
}
state_code_to_name2 = {
    "AC": "Acre", "AL": "Alagoas", "AP": "Amapá",
    "AM": "Amazonas", "BA": "Bahia", "CE": "Ceará",
    "DF": "Federal District", "ES": "Espírito Santo", "GO": "Goiás",
    "MA": "Maranhão", "MT": "Mato Grosso", "MS": "Mato Grosso do Sul",
    "MG": "Minas Gerais", "PA": "Pará", "PB": "Paraíba",
    "PR": "Paraná", "PE": "Pernambuco", "PI": "Piauí",
    "RJ": "Rio de Janeiro", "RN": "Rio Grande do Norte",
    "RS": "Rio Grande do Sul", "RO": "Rondônia", "RR": "Roraima",
    "SC": "Santa Catarina", "SP": "São Paulo", "SE": "Sergipe",
    "TO": "Tocantins"
}

# Define search terms and state codes
search_terms = [
    "sintomas dengue", "sintomas de dengue", "dengue",
    "mosquito da dengue", "febre do mosquito", "repelente contra dengue",
    "mosquito", "tratamento para dengue"
]
DEP_NAMES = {
    0: 'RO', 1: 'AC', 2: 'AM', 3: 'RR', 4: 'PA', 5: 'AP', 6: 'TO',
    7: 'MA', 8: 'PI', 9: 'CE', 10: 'RN', 11: 'PB', 12: 'PE', 
    13: 'AL', 14: 'SE', 15: 'BA', 16: 'MG', 17: 'ES', 18: 'RJ',
    19: 'SP', 20: 'PR', 21: 'SC', 22: 'RS', 23: 'MS', 24: 'MT',
    25: 'GO', 26: 'DF'
}

# Define the project path
project_path = os.getcwd()
project_path = os.path.join(project_path, "google trends", "google trends")

# Initialize counters and lists
mismatched_files = []
missing_combinations = []
all_combinations = [f"{dep}_{term}" for dep in DEP_NAMES.values() for term in search_terms]

# Process all files in the folder
files = os.listdir(project_path)
for file_name in files:
    if file_name.endswith(".csv"):
        file_path = os.path.join(project_path, file_name)
        try:
            # Load the CSV file
            df = pd.read_csv(file_path, index_col=0)
            df.reset_index(inplace=True)

            # Extract state code and category from the file name
            file_state_code, *file_category = file_name.split('.')[0].split('_')
            file_category = ' '.join(file_category)

            # Extract corresponding state name and category from the first entry
            first_entry = df.iloc[0, 1]  # Get the first value in the "Category: All categories" column
            entry_category, entry_state = first_entry.split(':', 1)
            entry_state = entry_state.strip(" ()")  # Remove parentheses and whitespace

            # Check for matches
            is_state_match = state_code_to_name.get(file_state_code) == entry_state or \
                             state_code_to_name2.get(file_state_code) == entry_state
            is_category_match = file_category.lower() == entry_category.strip().lower()

            # If overall match fails, add to the mismatched list
            if not (is_state_match and is_category_match):
                mismatched_files.append({
                    "file_name": file_name,
                    "first_entry": first_entry,
                    "state_match": is_state_match,
                    "category_match": is_category_match
                })
        except Exception as e:
            print(f"Error processing file {file_name}: {e}")

# Determine missing combinations
existing_combinations = [file_name.split('.')[0] for file_name in files if file_name.endswith(".csv")]
missing_combinations = [comb for comb in all_combinations if comb not in existing_combinations]

# Print results
print(f"\nTotal Files: {len(files)}")
print(f"Mismatched Files Count: {len(mismatched_files)}")
print("\nDetails of Mismatched Files:")
for mismatch in mismatched_files:
    print(mismatch)

print("\nMissing Combinations:")
print(missing_combinations)
len(missing_combinations)


from serpapi import GoogleSearch
import csv
import json

# Constants
state_code = "RO"  # Rondônia
search_term = "sintomas dengue"
timeframe = "2001-01-01 2019-12-01"  # Expanded timeframe
api_key = "35ae1bf2e3ed082a414e6351462f2cbf838ba4848a9758e14c4d475ca1bc2166"

# Output file for the test
output_file = "test_RO_sintomas_dengue.csv"

# Parameters for the SerpAPI request
params = {
    "engine": "google_trends",
    "q": search_term,
    "geo": f"BR-{state_code}",
    "hl": "pt",  # Portuguese for Brazil
    "date": timeframe,  # Adjusted key to match the parameter naming
    "api_key": api_key
}

# Fetch data from SerpAPI
try:
    print(f"Fetching data for State: {state_code}, Search Term: {search_term}")
    search = GoogleSearch(params)
    result = search.get_dict()  # Convert response to a dictionary

    # Debugging: Print the raw response to understand its structure
    print("Response received:")
    print(json.dumps(result, indent=4, ensure_ascii=False))

    # Extract interest over time data
    interest_data = result.get("interest_over_time", {}).get("timeline_data", [])
    
    if interest_data:
        # Save results to a CSV file
        with open(output_file, mode="w", newline="", encoding="utf-8") as file:
            writer = csv.writer(file)
            writer.writerow(["Date", "Interest"])
            
            for entry in interest_data:
                date = entry.get("date")
                values = entry.get("values", [])
                partial_data = entry.get("partial_data", False)
                
                # Only process non-partial weeks with valid values
                if not partial_data and values:
                    interest_value = values[0].get("value", 0)
                    writer.writerow([date, interest_value])
        
        print(f"Data saved to {output_file}")
    else:
        print("No data found for the specified query. Check if the term or geo is valid.")
except Exception as e:
    print(f"An error occurred: {e}")


earch_terms = [
    "sintomas dengue", "sintomas de dengue", "dengue",
    "mosquito da dengue", "febre do mosquito", "repelente contra dengue",
    "mosquito", "tratamento para dengue"
]
DEP_NAMES = {
    0: 'RO', 1: 'AC', 2: 'AM', 3: 'RR', 4: 'PA', 5: 'AP', 6: 'TO',
    7: 'MA', 8: 'PI', 9: 'CE', 10: 'RN', 11: 'PB', 
    # 12: 'PE', 
    # 13: 'AL', 14: 'SE', 15: 'BA', 16: 'MG', 17: 'ES', 18: 'RJ',
    # 19: 'SP', 20: 'PR', 21: 'SC', 22: 'RS', 23: 'MS', 24: 'MT',
    # 25: 'GO', 26: 'DF'
}


import os
import csv
import json
from serpapi import GoogleSearch

# Constants
search_terms = [
    "sintomas dengue", "sintomas de dengue", "dengue",
    "mosquito da dengue", "febre do mosquito", "repelente contra dengue",
    "mosquito", "tratamento para dengue"
]
DEP_NAMES = {
    0: 'RO', 1: 'AC', 2: 'AM', 3: 'RR', 4: 'PA', 5: 'AP', 6: 'TO',
    7: 'MA', 8: 'PI', 9: 'CE', 10: 'RN', 11: 'PB'
}
timeframe = "2001-01-01 2019-12-01"  # Expanded timeframe
api_key = "35ae1bf2e3ed082a414e6351462f2cbf838ba4848a9758e14c4d475ca1bc2166"

# Create a folder to save results
output_folder = "serpapi_results"
os.makedirs(output_folder, exist_ok=True)

# Initialize a list to track missing data combinations
missing_data_combinations = []

# Process each state and each search term
for state_code in DEP_NAMES.values():
    for search_term in search_terms:
        # Parameters for the SerpAPI request
        params = {
            "engine": "google_trends",
            "q": search_term,
            "geo": f"BR-{state_code}",
            "hl": "pt",  # Portuguese for Brazil
            "date": timeframe,
            "api_key": api_key
        }

        # Output file path
        output_file = os.path.join(output_folder, f"{state_code}_{search_term.replace(' ', '_')}.csv")

        # Fetch data from SerpAPI
        try:
            print(f"Fetching data for State: {state_code}, Search Term: {search_term}")
            search = GoogleSearch(params)
            result = search.get_dict()  # Convert response to a dictionary

            # Debugging: Print the raw response to understand its structure
            print("Response received:")
            # print(json.dumps(result, indent=4, ensure_ascii=False))

            # Extract interest over time data
            interest_data = result.get("interest_over_time", {}).get("timeline_data", [])
            
            if interest_data:
                # Save results to a CSV file
                with open(output_file, mode="w", newline="", encoding="utf-8") as file:
                    writer = csv.writer(file)
                    writer.writerow(["Date", "Interest"])
                    
                    for entry in interest_data:
                        date = entry.get("date")
                        values = entry.get("values", [])
                        partial_data = entry.get("partial_data", False)
                        
                        # Only process non-partial weeks with valid values
                        if not partial_data and values:
                            interest_value = values[0].get("value", 0)
                            writer.writerow([date, interest_value])
                
                print(f"Data saved to {output_file}")
            else:
                print(f"No data found for State: {state_code}, Search Term: {search_term}")
                missing_data_combinations.append((state_code, search_term))
        except Exception as e:
            print(f"An error occurred for State: {state_code}, Search Term: {search_term}: {e}")
            missing_data_combinations.append((state_code, search_term))

# Log missing data combinations
if missing_data_combinations:
    missing_file = os.path.join(output_folder, "missing_data_log.txt")
    with open(missing_file, "w", encoding="utf-8") as log_file:
        log_file.write("No data found for the following State-Search Term combinations:\n")
        for state, term in missing_data_combinations:
            log_file.write(f"State: {state}, Search Term: {term}\n")
    print(f"Missing data logged to {missing_file}")
else:
    print("All data successfully fetched and saved.")



import os
import csv
import json
from serpapi import GoogleSearch

# Constants
search_terms = [
    "sintomas dengue", "sintomas de dengue", "dengue",
    "mosquito da dengue", "febre do mosquito", "repelente contra dengue",
    "mosquito", "tratamento para dengue"
]
DEP_NAMES = {
    12: 'PE', 13: 'AL', 14: 'SE', 15: 'BA', 16: 'MG', 17: 'ES', 18: 'RJ',
    19: 'SP', 20: 'PR', 21: 'SC', 22: 'RS'
}


timeframe = "2001-01-01 2019-12-01"  # Expanded timeframe
api_key = "0bbeb515833bea08369ee736c9cfba648cea12fea30afacc28238b78e30382e5"

# Create a folder to save results
output_folder = "serpapi_results"
os.makedirs(output_folder, exist_ok=True)

# Initialize a list to track missing data combinations
missing_data_combinations = []

# Process each state and each search term
for state_code in DEP_NAMES.values():
    for search_term in search_terms:
        # Parameters for the SerpAPI request
        params = {
            "engine": "google_trends",
            "q": search_term,
            "geo": f"BR-{state_code}",
            "hl": "pt",  # Portuguese for Brazil
            "date": timeframe,
            "api_key": api_key
        }

        # Output file path
        output_file = os.path.join(output_folder, f"{state_code}_{search_term.replace(' ', '_')}.csv")

        # Fetch data from SerpAPI
        try:
            print(f"Fetching data for State: {state_code}, Search Term: {search_term}")
            search = GoogleSearch(params)
            result = search.get_dict()  # Convert response to a dictionary

            # Debugging: Print the raw response to understand its structure
            print("Response received:")
            # print(json.dumps(result, indent=4, ensure_ascii=False))

            # Extract interest over time data
            interest_data = result.get("interest_over_time", {}).get("timeline_data", [])
            
            if interest_data:
                # Save results to a CSV file
                with open(output_file, mode="w", newline="", encoding="utf-8") as file:
                    writer = csv.writer(file)
                    writer.writerow(["Date", "Interest"])
                    
                    for entry in interest_data:
                        date = entry.get("date")
                        values = entry.get("values", [])
                        partial_data = entry.get("partial_data", False)
                        
                        # Only process non-partial weeks with valid values
                        if not partial_data and values:
                            interest_value = values[0].get("value", 0)
                            writer.writerow([date, interest_value])
                
                print(f"Data saved to {output_file}")
            else:
                print(f"No data found for State: {state_code}, Search Term: {search_term}")
                missing_data_combinations.append((state_code, search_term))
        except Exception as e:
            print(f"An error occurred for State: {state_code}, Search Term: {search_term}: {e}")
            missing_data_combinations.append((state_code, search_term))

# Log missing data combinations
if missing_data_combinations:
    missing_file = os.path.join(output_folder, "missing_data_log.txt")
    with open(missing_file, "w", encoding="utf-8") as log_file:
        log_file.write("No data found for the following State-Search Term combinations:\n")
        for state, term in missing_data_combinations:
            log_file.write(f"State: {state}, Search Term: {term}\n")
    print(f"Missing data logged to {missing_file}")
else:
    print("All data successfully fetched and saved.")



import os
import csv
import json
from serpapi import GoogleSearch

# Constants
search_terms = [
    "sintomas dengue", "sintomas de dengue", "dengue",
    "mosquito da dengue", "febre do mosquito", "repelente contra dengue",
    "mosquito", "tratamento para dengue"
]
DEP_NAMES = {
    23: 'MS', 24: 'MT',
    25: 'GO', 26: 'DF'
}


timeframe = "2001-01-01 2019-12-01"  # Expanded timeframe
api_key = "7e8d78c24551d970db3f53487e45a82c232180250e17fab6a5eede7c3ee0c70c"

# Create a folder to save results
output_folder = "serpapi_results"
os.makedirs(output_folder, exist_ok=True)

# Initialize a list to track missing data combinations
missing_data_combinations = []

# Process each state and each search term
for state_code in DEP_NAMES.values():
    for search_term in search_terms:
        # Parameters for the SerpAPI request
        params = {
            "engine": "google_trends",
            "q": search_term,
            "geo": f"BR-{state_code}",
            "hl": "pt",  # Portuguese for Brazil
            "date": timeframe,
            "api_key": api_key
        }

        # Output file path
        output_file = os.path.join(output_folder, f"{state_code}_{search_term.replace(' ', '_')}.csv")

        # Fetch data from SerpAPI
        try:
            print(f"Fetching data for State: {state_code}, Search Term: {search_term}")
            search = GoogleSearch(params)
            result = search.get_dict()  # Convert response to a dictionary

            # Debugging: Print the raw response to understand its structure
            print("Response received:")
            # print(json.dumps(result, indent=4, ensure_ascii=False))

            # Extract interest over time data
            interest_data = result.get("interest_over_time", {}).get("timeline_data", [])
            
            if interest_data:
                # Save results to a CSV file
                with open(output_file, mode="w", newline="", encoding="utf-8") as file:
                    writer = csv.writer(file)
                    writer.writerow(["Date", "Interest"])
                    
                    for entry in interest_data:
                        date = entry.get("date")
                        values = entry.get("values", [])
                        partial_data = entry.get("partial_data", False)
                        
                        # Only process non-partial weeks with valid values
                        if not partial_data and values:
                            interest_value = values[0].get("value", 0)
                            writer.writerow([date, interest_value])
                
                print(f"Data saved to {output_file}")
            else:
                print(f"No data found for State: {state_code}, Search Term: {search_term}")
                missing_data_combinations.append((state_code, search_term))
        except Exception as e:
            print(f"An error occurred for State: {state_code}, Search Term: {search_term}: {e}")
            missing_data_combinations.append((state_code, search_term))

# Log missing data combinations
if missing_data_combinations:
    missing_file = os.path.join(output_folder, "missing_data_log.txt")
    with open(missing_file, "w", encoding="utf-8") as log_file:
        log_file.write("No data found for the following State-Search Term combinations:\n")
        for state, term in missing_data_combinations:
            log_file.write(f"State: {state}, Search Term: {term}\n")
    print(f"Missing data logged to {missing_file}")
else:
    print("All data successfully fetched and saved.")



# List of files to process
file_paths = ["../serpapi_results/missing_data_log.txt", "../serpapi_results/missing_data_log1.txt", "../serpapi_results/missing_data_log2.txt"]

# Dictionaries to store total frequencies
total_state_frequency = {}
total_search_term_frequency = {}
total_combination_frequency = {}

# Process each file
for file_path in file_paths:
    with open(file_path, 'r') as file:
        for line in file:
            if line.startswith("State:"):
                # Extract state and search term
                parts = line.strip().split(", ")
                state = parts[0].split(": ")[1]
                search_term = parts[1].split(": ")[1]

                # Update total frequencies for states
                total_state_frequency[state] = total_state_frequency.get(state, 0) + 1

                # Update total frequencies for search terms
                total_search_term_frequency[search_term] = total_search_term_frequency.get(search_term, 0) + 1

                # Update total frequencies for combinations
                combination = f"{state} - {search_term}"
                total_combination_frequency[combination] = total_combination_frequency.get(combination, 0) + 1

# Print the total frequencies
print("Total State Frequencies:")
for state, freq in total_state_frequency.items():
    print(f"{state}: {freq}")

print("\nTotal Search Term Frequencies:")
for search_term, freq in total_search_term_frequency.items():
    print(f"{search_term}: {freq}")

print("\nTotal State-Search Term Combination Frequencies:")
for combination, freq in total_combination_frequency.items():
    print(f"{combination}: {freq}")

print("\n")
# List of search terms to filter
filter_search_terms = {"sintomas dengue", "mosquito da dengue", "sintomas de dengue", "mosquito"}

# Filter combinations based on exact search term match
filtered_combinations = {
    combination: freq
    for combination, freq in total_combination_frequency.items()
    if combination.split(" - ")[1] in filter_search_terms  # Match exact search term
}

# Print filtered combinations
print("Filtered Combinations (Exact Search Term Matches):")
for combination, freq in filtered_combinations.items():
    print(f"{combination}: {freq}")



import os
import shutil

# Constants
current_folder = "../serpapi_results"  # Replace with the folder where the files are stored
new_folder = "excluded_terms"
excluded_terms = ["febre do mosquito", "repelente contra dengue", "tratamento para dengue"]

# Create a folder for excluded terms
os.makedirs(new_folder, exist_ok=True)

# Process files in the current folder
for file_name in os.listdir(current_folder):
    # Check if the file corresponds to an excluded term
    for term in excluded_terms:
        sanitized_term = term.replace(" ", "_")
        if sanitized_term in file_name:
            # Move file to the new folder
            source_path = os.path.join(current_folder, file_name)
            destination_path = os.path.join(new_folder, file_name)
            shutil.move(source_path, destination_path)
            print(f"Moved: {file_name} -> {new_folder}")

print("File movement completed.")



import os
import shutil

# Constants
current_folder = "../serpapi_results"  # Replace with the folder where the files are stored
new_folder = "missing_logs"
excluded_terms = ["missing_data_log.txt", "missing_data_log1.txt", "missing_data_log2.txt"]

# Create a folder for excluded terms
os.makedirs(new_folder, exist_ok=True)

# Process files in the current folder
for file_name in os.listdir(current_folder):
    # Check if the file corresponds to an excluded term
    for term in excluded_terms:
        # sanitized_term = term.replace(" ", "_")
        if term in file_name:
            # Move file to the new folder
            source_path = os.path.join(current_folder, file_name)
            destination_path = os.path.join(new_folder, file_name)
            shutil.move(source_path, destination_path)
            print(f"Moved: {file_name} -> {new_folder}")

print("File movement completed.")


import os
import pandas as pd

# Directory containing the CSV files
input_folder = "../serpapi_results"

# Dictionary to store results
filewise_counts = {}
search_term_wise_counts = {}

print("Filewise Counts:")
# Process each CSV file in the folder
for file_name in os.listdir(input_folder):
    if file_name.endswith(".csv"):
        file_path = os.path.join(input_folder, file_name)
        df = pd.read_csv(file_path)
        
        # Count zero and non-zero values
        count_zeros = len(df[df.Interest == 0])
        count_non_zeros = len(df[df.Interest != 0])
        
        # Extract the search term from the file name
        search_term = "_".join(file_name.split("_")[1:]).replace(".csv", "")
        
        # Accumulate search-term-wise counts
        if search_term not in search_term_wise_counts:
            search_term_wise_counts[search_term] = {"Zero Count": 0, "Non-Zero Count": 0, "File Count": 0}
        
        search_term_wise_counts[search_term]["Zero Count"] += count_zeros
        search_term_wise_counts[search_term]["Non-Zero Count"] += count_non_zeros
        search_term_wise_counts[search_term]["File Count"] += 1
        
        # Print file-wise counts
        print(f"{file_name}: zeroes: {count_zeros}, not-zero: {count_non_zeros}")

# Calculate and print averages for each search term
print("\nSearch-Term-Wise Averages:")
for search_term, counts in search_term_wise_counts.items():
    avg_zeros = counts["Zero Count"] / counts["File Count"]
    avg_non_zeros = counts["Non-Zero Count"] / counts["File Count"]
    print(f"{search_term}: Average zeroes: {avg_zeros:.2f}, Average non-zeroes: {avg_non_zeros:.2f}")



df=pd.read_csv('../code/reduced.csv')
df.dep_id.unique()


import os
import pandas as pd

# Map for converting Portuguese month abbreviations to numbers
month_map = {
    "jan.": "01", "fev.": "02", "mar.": "03", "abr.": "04", "mai.": "05", "jun.": "06",
    "jul.": "07", "ago.": "08", "set.": "09", "out.": "10", "nov.": "11", "dez.": "12"
}

# Folder containing the CSV files
input_folder = "../serpapi_results"
output_folder = "../serpapi_results_converted"
os.makedirs(output_folder, exist_ok=True)

# Process each CSV file
for file_name in os.listdir(input_folder):
    if file_name.endswith(".csv"):
        file_path = os.path.join(input_folder, file_name)
        df = pd.read_csv(file_path)
        
        # Convert the Date column
        def convert_date(date_str):
            month_abbr, year = date_str.split(" ")
            month = month_map[month_abbr]
            return f"{year}-{month}"
        
        df["Date"] = df["Date"].apply(convert_date)
        
        # Save the updated DataFrame to a new file
        output_path = os.path.join(output_folder, file_name)
        df.to_csv(output_path, index=False, encoding="utf-8")
        print(f"Processed and saved: {output_path}")



import os

# Output folder containing converted files
output_folder = "../serpapi_results_converted"

# Search terms and department names
search_terms = ["sintomas dengue", "dengue", "mosquito"]
DEP_NAMES = {
    0: 'RO', 1: 'AC', 2: 'AM', 3: 'RR', 4: 'PA', 5: 'AP', 6: 'TO',
    7: 'MA', 8: 'PI', 9: 'CE', 10: 'RN', 11: 'PB', 12: 'PE',
    13: 'AL', 14: 'SE', 15: 'BA', 16: 'MG', 17: 'ES', 18: 'RJ',
    19: 'SP', 20: 'PR', 21: 'SC', 22: 'RS', 23: 'MS', 24: 'MT',
    25: 'GO', 26: 'DF'
}

# Generate all expected file combinations
expected_files = [f"{dep}_{term.replace(' ', '_')}.csv" for dep in DEP_NAMES.values() for term in search_terms]

# List files in the output folder
existing_files = os.listdir(output_folder)

# Check for missing files
missing_files = [file for file in expected_files if file not in existing_files]

# Print missing combinations
print("Missing File Combinations:")
for file in missing_files:
    print(file)



import os
import csv
import json
from serpapi import GoogleSearch

# Constants
search_terms = [
    "mosquito"
]
DEP_NAMES = {
    9: 'CE'
}
timeframe = "2004-01-01 2019-12-01"  # Expanded timeframe
api_key = "7e8d78c24551d970db3f53487e45a82c232180250e17fab6a5eede7c3ee0c70c"

# Create a folder to save results
output_folder = "serpapi_results_new"
os.makedirs(output_folder, exist_ok=True)

# Initialize a list to track missing data combinations
missing_data_combinations = []

# Process each state and each search term
for state_code in DEP_NAMES.values():
    for search_term in search_terms:
        # Parameters for the SerpAPI request
        params = {
            "engine": "google_trends",
            "q": search_term,
            "geo": f"BR-{state_code}",
            "hl": "pt",  # Portuguese for Brazil
            "date": timeframe,
            "api_key": api_key
        }

        # Output file path
        output_file = os.path.join(output_folder, f"{state_code}_{search_term.replace(' ', '_')}.csv")

        # Fetch data from SerpAPI
        try:
            print(f"Fetching data for State: {state_code}, Search Term: {search_term}")
            search = GoogleSearch(params)
            result = search.get_dict()  # Convert response to a dictionary

            # Debugging: Print the raw response to understand its structure
            print("Response received:")
            # print(json.dumps(result, indent=4, ensure_ascii=False))

            # Extract interest over time data
            interest_data = result.get("interest_over_time", {}).get("timeline_data", [])
            
            if interest_data:
                # Save results to a CSV file
                with open(output_file, mode="w", newline="", encoding="utf-8") as file:
                    writer = csv.writer(file)
                    writer.writerow(["Date", "Interest"])
                    
                    for entry in interest_data:
                        date = entry.get("date")
                        values = entry.get("values", [])
                        partial_data = entry.get("partial_data", False)
                        
                        # Only process non-partial weeks with valid values
                        if not partial_data and values:
                            interest_value = values[0].get("value", 0)
                            writer.writerow([date, interest_value])
                
                print(f"Data saved to {output_file}")
            else:
                print(f"No data found for State: {state_code}, Search Term: {search_term}")
                missing_data_combinations.append((state_code, search_term))
        except Exception as e:
            print(f"An error occurred for State: {state_code}, Search Term: {search_term}: {e}")
            missing_data_combinations.append((state_code, search_term))

# Log missing data combinations
if missing_data_combinations:
    missing_file = os.path.join(output_folder, "missing_data_log.txt")
    with open(missing_file, "w", encoding="utf-8") as log_file:
        log_file.write("No data found for the following State-Search Term combinations:\n")
        for state, term in missing_data_combinations:
            log_file.write(f"State: {state}, Search Term: {term}\n")
    print(f"Missing data logged to {missing_file}")
else:
    print("All data successfully fetched and saved.")



import os
import pandas as pd

# Input folder containing the CSV files
input_folder = "../serpapi_results_converted"
output_folder = "../serpapi_results_cleaned"

# Ensure the output folder exists
os.makedirs(output_folder, exist_ok=True)

# Process each file in the input folder
for file_name in os.listdir(input_folder):
    if file_name.endswith(".csv"):
        file_path = os.path.join(input_folder, file_name)
        
        try:
            # Read the file
            df = pd.read_csv(file_path)

            # Check for "<1" and replace it with 0.5
            df["Interest"] = df["Interest"].replace("<1", 0.5)

            # Convert Interest column to float
            df["Interest"] = df["Interest"].astype(float)

            # Save the cleaned file to the output folder
            output_path = os.path.join(output_folder, file_name)
            df.to_csv(output_path, index=False)
            print(f"Processed and saved: {file_name}")

        except Exception as e:
            print(f"Error processing {file_name}: {e}")



import os
import shutil

# Input folder containing all the files
input_folder = "../serpapi_results_cleaned"
output_folder = "../serpapi_results_filtered"

# Ensure the output folder exists
os.makedirs(output_folder, exist_ok=True)

# Search terms to keep
keep_terms = ["dengue", "mosquito", "sintomas dengue"]

# Search terms to filter out
exclude_terms = [
    "sintomas de dengue", "mosquito da dengue", "febre do mosquito",
    "repelente contra dengue", "tratamento para dengue"
]

# Process each file
for file_name in os.listdir(input_folder):
    # Check if the filename matches any of the terms to keep
    if any(term.replace(" ", "_") in file_name for term in keep_terms):
        # Ensure it doesn't match any of the exclude terms
        if not any(term.replace(" ", "_") in file_name for term in exclude_terms):
            source_path = os.path.join(input_folder, file_name)
            destination_path = os.path.join(output_folder, file_name)
            
            # Copy the file to the filtered folder
            shutil.copy(source_path, destination_path)
            print(f"Copied: {file_name}")

print(f"Filtered files saved to: {output_folder}")



import os
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.seasonal import seasonal_decompose

# Input folder containing the CSV files
input_folder = "../serpapi_results_filtered"

# Parameters
decomposition_model = "additive"  # Choose between 'additive' or 'multiplicative'
period = 12  # Monthly data, so period = 12

# Group data for visualization
state_data = {}
search_term_data = {}

# Process each file
for file_name in os.listdir(input_folder):
    if file_name.endswith(".csv"):
        # Extract state and search term from file name
        file_path = os.path.join(input_folder, file_name)
        state = file_name.split("_")[0]  # Assuming state is the first part
        search_term = file_name.split("_", 1)[1].replace(".csv", "")  # Rest is search term

        # Read the file
        try:
            data = pd.read_csv(file_path, parse_dates=["Date"], index_col="Date")
            data["Interest"] = pd.to_numeric(data["Interest"], errors="coerce")

            # Perform decomposition
            decomposition = seasonal_decompose(data["Interest"].dropna(), model=decomposition_model, period=period)

            # Store components for visualization
            state_data.setdefault(state, []).append(decomposition)
            search_term_data.setdefault(search_term, []).append(decomposition)

            # Visualize decomposition (state-wise)
            plt.figure(figsize=(14, 10))
            plt.suptitle(f"{state} - {search_term} Decomposition", fontsize=16)

            plt.subplot(411)
            plt.plot(decomposition.observed, label="Observed")
            plt.legend(loc="best")
            plt.subplot(412)
            plt.plot(decomposition.trend, label="Trend")
            plt.legend(loc="best")
            plt.subplot(413)
            plt.plot(decomposition.seasonal, label="Seasonality")
            plt.legend(loc="best")
            plt.subplot(414)
            plt.plot(decomposition.resid, label="Residual")
            plt.legend(loc="best")

            plt.tight_layout(rect=[0, 0, 1, 0.96])
            plt.show()

        except Exception as e:
            print(f"Error processing file {file_name}: {e}")

# Combined plots for states (grouped across search terms)
for state, decompositions in state_data.items():
    plt.figure(figsize=(14, 10))
    plt.suptitle(f"{state} - Combined Search Terms Decomposition", fontsize=16)
    for i, decomposition in enumerate(decompositions):
        plt.subplot(len(decompositions), 1, i + 1)
        plt.plot(decomposition.trend, label=f"Trend ({state})")
        plt.plot(decomposition.seasonal, label="Seasonal", linestyle="--")
        plt.legend(loc="best")
    plt.tight_layout(rect=[0, 0, 1, 0.96])
    plt.show()

# Combined plots for search terms (grouped across states)
for search_term, decompositions in search_term_data.items():
    plt.figure(figsize=(14, 10))
    plt.suptitle(f"{search_term} - Combined States Decomposition", fontsize=16)
    for i, decomposition in enumerate(decompositions):
        plt.subplot(len(decompositions), 1, i + 1)
        plt.plot(decomposition.trend, label=f"Trend ({search_term})")
        plt.plot(decomposition.seasonal, label="Seasonal", linestyle="--")
        plt.legend(loc="best")
    plt.tight_layout(rect=[0, 0, 1, 0.96])
    plt.show()



import pandas as pd
import os
from sklearn.preprocessing import MinMaxScaler

# Load the main dataset
main_data_path = '../code/reduced_2004_2019.csv'
main_data = pd.read_csv(main_data_path)

# Ensure correct parsing of Month and Date columns in the main dataset
main_data['Month'] = (main_data.groupby(['Year']).cumcount() % 12 + 1)  # Generate month if not present
main_data['Date'] = pd.to_datetime(main_data['Year'].astype(str) + '-' + main_data['Month'].astype(str).str.zfill(2))

# Mapping of Brazilian states
brazil_states = {
    11: {"name": "Rondônia", "code": "RO"}, 12: {"name": "Acre", "code": "AC"}, 13: {"name": "Amazonas", "code": "AM"},
    14: {"name": "Roraima", "code": "RR"}, 15: {"name": "Pará", "code": "PA"}, 16: {"name": "Amapá", "code": "AP"},
    17: {"name": "Tocantins", "code": "TO"}, 21: {"name": "Maranhão", "code": "MA"}, 22: {"name": "Piauí", "code": "PI"},
    23: {"name": "Ceará", "code": "CE"}, 24: {"name": "Rio Grande do Norte", "code": "RN"},
    25: {"name": "Paraíba", "code": "PB"}, 26: {"name": "Pernambuco", "code": "PE"}, 27: {"name": "Alagoas", "code": "AL"},
    28: {"name": "Sergipe", "code": "SE"}, 29: {"name": "Bahia", "code": "BA"}, 31: {"name": "Minas Gerais", "code": "MG"},
    32: {"name": "Espírito Santo", "code": "ES"}, 33: {"name": "Rio de Janeiro", "code": "RJ"},
    35: {"name": "São Paulo", "code": "SP"}, 41: {"name": "Paraná", "code": "PR"},
    42: {"name": "Santa Catarina", "code": "SC"}, 43: {"name": "Rio Grande do Sul", "code": "RS"},
    50: {"name": "Mato Grosso do Sul", "code": "MS"}, 51: {"name": "Mato Grosso", "code": "MT"},
    52: {"name": "Goiás", "code": "GO"}, 53: {"name": "Distrito Federal", "code": "DF"}
}

# Reverse map for quick lookup
state_code_to_dep_id = {v['code']: k for k, v in brazil_states.items()}

# List of trend files to process
trends_files = [
    '../serpapi_results_filtered/AC_mosquito.csv',
    '../serpapi_results_filtered/AC_sintomas_dengue.csv',
    '../serpapi_results_filtered/AC_dengue.csv',
    '../serpapi_results_filtered/AM_mosquito.csv',
    '../serpapi_results_filtered/AM_sintomas_dengue.csv',
    '../serpapi_results_filtered/AM_dengue.csv',
    '../serpapi_results_filtered/CE_mosquito.csv',
    '../serpapi_results_filtered/AL_mosquito.csv',
    '../serpapi_results_filtered/AL_sintomas_dengue.csv',
    '../serpapi_results_filtered/AL_dengue.csv',
    '../serpapi_results_filtered/BA_mosquito.csv',
    '../serpapi_results_filtered/BA_sintomas_dengue.csv',
    '../serpapi_results_filtered/BA_dengue.csv',
    '../serpapi_results_filtered/AP_mosquito.csv',
    '../serpapi_results_filtered/AP_sintomas_dengue.csv',
    '../serpapi_results_filtered/AP_dengue.csv',
    '../serpapi_results_filtered/CE_sintomas_dengue.csv',
    '../serpapi_results_filtered/CE_dengue.csv',
    '../serpapi_results_filtered/DF_mosquito.csv',
    '../serpapi_results_filtered/DF_sintomas_dengue.csv',
    '../serpapi_results_filtered/DF_dengue.csv',
    '../serpapi_results_filtered/RO_mosquito.csv',
    '../serpapi_results_filtered/RO_sintomas_dengue.csv',
    '../serpapi_results_filtered/RO_dengue.csv',
    '../serpapi_results_filtered/RR_mosquito.csv',
    '../serpapi_results_filtered/RR_sintomas_dengue.csv',
    '../serpapi_results_filtered/RR_dengue.csv',
    '../serpapi_results_filtered/PA_mosquito.csv',
    '../serpapi_results_filtered/PA_sintomas_dengue.csv',
    '../serpapi_results_filtered/PA_dengue.csv',
    '../serpapi_results_filtered/GO_mosquito.csv',
    '../serpapi_results_filtered/GO_sintomas_dengue.csv',
    '../serpapi_results_filtered/GO_dengue.csv',
    '../serpapi_results_filtered/ES_mosquito.csv',
    '../serpapi_results_filtered/ES_sintomas_dengue.csv',
    '../serpapi_results_filtered/ES_dengue.csv',
    '../serpapi_results_filtered/TO_mosquito.csv',
    '../serpapi_results_filtered/TO_sintomas_dengue.csv',
    '../serpapi_results_filtered/TO_dengue.csv',
    '../serpapi_results_filtered/MA_mosquito.csv',
    '../serpapi_results_filtered/MA_sintomas_dengue.csv',
    '../serpapi_results_filtered/MA_dengue.csv',
    '../serpapi_results_filtered/PI_mosquito.csv',
    '../serpapi_results_filtered/PI_sintomas_dengue.csv',
    '../serpapi_results_filtered/PI_dengue.csv',
    '../serpapi_results_filtered/RN_mosquito.csv',
    '../serpapi_results_filtered/RN_sintomas_dengue.csv',
    '../serpapi_results_filtered/RN_dengue.csv',
    '../serpapi_results_filtered/PB_mosquito.csv',
    '../serpapi_results_filtered/PB_sintomas_dengue.csv',
    '../serpapi_results_filtered/PB_dengue.csv',
    '../serpapi_results_filtered/PE_mosquito.csv',
    '../serpapi_results_filtered/PE_sintomas_dengue.csv',
    '../serpapi_results_filtered/PE_dengue.csv',
    '../serpapi_results_filtered/MG_mosquito.csv',
    '../serpapi_results_filtered/MG_sintomas_dengue.csv',
    '../serpapi_results_filtered/MG_dengue.csv',
    '../serpapi_results_filtered/SE_mosquito.csv',
    '../serpapi_results_filtered/SE_sintomas_dengue.csv',
    '../serpapi_results_filtered/SE_dengue.csv',
    '../serpapi_results_filtered/MS_mosquito.csv',
    '../serpapi_results_filtered/MS_sintomas_dengue.csv',
    '../serpapi_results_filtered/MS_dengue.csv',
    '../serpapi_results_filtered/MT_mosquito.csv',
    '../serpapi_results_filtered/MT_sintomas_dengue.csv',
    '../serpapi_results_filtered/MT_dengue.csv',
    '../serpapi_results_filtered/PR_mosquito.csv',
    '../serpapi_results_filtered/PR_sintomas_dengue.csv',
    '../serpapi_results_filtered/PR_dengue.csv',
    '../serpapi_results_filtered/RJ_mosquito.csv',
    '../serpapi_results_filtered/RJ_sintomas_dengue.csv',
    '../serpapi_results_filtered/RJ_dengue.csv',
    '../serpapi_results_filtered/RS_mosquito.csv',
    '../serpapi_results_filtered/RS_sintomas_dengue.csv',
    '../serpapi_results_filtered/RS_dengue.csv',
    '../serpapi_results_filtered/SC_mosquito.csv',
    '../serpapi_results_filtered/SC_sintomas_dengue.csv',
    '../serpapi_results_filtered/SC_dengue.csv',
    '../serpapi_results_filtered/SP_mosquito.csv',
    '../serpapi_results_filtered/SP_sintomas_dengue.csv',
    '../serpapi_results_filtered/SP_dengue.csv'
]

# Initialize trend columns in the main dataset
search_terms = ['mosquito', 'sintomas_dengue', 'dengue']
for term in search_terms:
    main_data[f'{term}_interest'] = pd.NA

# Iterate over the trend files and merge data
for file_name in trends_files:
    # Extract state code and search term from file name
    base_name = os.path.basename(file_name)  # e.g., 'AC_mosquito.csv'
    state_code, search_term = base_name.split('_')[0], '_'.join(base_name.split('_')[1:]).replace('.csv', '')
    dep_id = state_code_to_dep_id.get(state_code)  # Map state code to dep_id
   
    if not dep_id:
        print(f"Skipping file {file_name} (state code {state_code} not found)")
        continue

    # Load trends data
    trends_data = pd.read_csv(file_name)

    # Ensure Date column is in datetime format
    trends_data['Date'] = pd.to_datetime(trends_data['Date'], format='%Y-%m')

    # Rename 'Interest' column to include the search term
    trends_data = trends_data.rename(columns={'Interest': f'{search_term}_interest'})

    # Merge trends data with the main dataset
    main_data = pd.merge(
        main_data,
        trends_data[['Date', f'{search_term}_interest']],
        on=['Date'],
        how='left',
        suffixes=('', f'_{state_code}')
    )

    # Apply dep_id condition to keep relevant rows
    main_data.loc[main_data['dep_id'] == dep_id, f'{search_term}_interest'] = main_data[f'{search_term}_interest_{state_code}']

    # Drop intermediate column
    main_data.drop(columns=[f'{search_term}_interest_{state_code}'], inplace=True)
# Save the final merged dataset
main_data.to_csv('merged_dataset.csv', index=False)
print("Merged dataset saved as 'merged_dataset.csv'")


import pandas as pd
import os
from sklearn.preprocessing import MinMaxScaler

# Load the main dataset
main_data_path = '../code/reduced_2004_2019.csv'
main_data = pd.read_csv(main_data_path)

# Ensure correct parsing of Month and Date columns in the main dataset
main_data['Month'] = (main_data.groupby(['Year']).cumcount() % 12 + 1)  # Generate month if not present
main_data['Date'] = pd.to_datetime(main_data['Year'].astype(str) + '-' + main_data['Month'].astype(str).str.zfill(2))

# Mapping of Brazilian states
brazil_states = {
    11: {"name": "Rondônia", "code": "RO"}, 12: {"name": "Acre", "code": "AC"}, 13: {"name": "Amazonas", "code": "AM"},
    14: {"name": "Roraima", "code": "RR"}, 15: {"name": "Pará", "code": "PA"}, 16: {"name": "Amapá", "code": "AP"},
    17: {"name": "Tocantins", "code": "TO"}, 21: {"name": "Maranhão", "code": "MA"}, 22: {"name": "Piauí", "code": "PI"},
    23: {"name": "Ceará", "code": "CE"}, 24: {"name": "Rio Grande do Norte", "code": "RN"},
    25: {"name": "Paraíba", "code": "PB"}, 26: {"name": "Pernambuco", "code": "PE"}, 27: {"name": "Alagoas", "code": "AL"},
    28: {"name": "Sergipe", "code": "SE"}, 29: {"name": "Bahia", "code": "BA"}, 31: {"name": "Minas Gerais", "code": "MG"},
    32: {"name": "Espírito Santo", "code": "ES"}, 33: {"name": "Rio de Janeiro", "code": "RJ"},
    35: {"name": "São Paulo", "code": "SP"}, 41: {"name": "Paraná", "code": "PR"},
    42: {"name": "Santa Catarina", "code": "SC"}, 43: {"name": "Rio Grande do Sul", "code": "RS"},
    50: {"name": "Mato Grosso do Sul", "code": "MS"}, 51: {"name": "Mato Grosso", "code": "MT"},
    52: {"name": "Goiás", "code": "GO"}, 53: {"name": "Distrito Federal", "code": "DF"}
}

# Reverse map for quick lookup
state_code_to_dep_id = {v['code']: k for k, v in brazil_states.items()}

# List of trend files to process
trends_files = [
    '../serpapi_results_filtered/AC_mosquito.csv',
    '../serpapi_results_filtered/AC_sintomas_dengue.csv',
    '../serpapi_results_filtered/AC_dengue.csv',
    '../serpapi_results_filtered/AM_mosquito.csv',
    '../serpapi_results_filtered/AM_sintomas_dengue.csv',
    '../serpapi_results_filtered/AM_dengue.csv',
    '../serpapi_results_filtered/CE_mosquito.csv',
    '../serpapi_results_filtered/AL_mosquito.csv',
    '../serpapi_results_filtered/AL_sintomas_dengue.csv',
    '../serpapi_results_filtered/AL_dengue.csv',
    '../serpapi_results_filtered/BA_mosquito.csv',
    '../serpapi_results_filtered/BA_sintomas_dengue.csv',
    '../serpapi_results_filtered/BA_dengue.csv',
    '../serpapi_results_filtered/AP_mosquito.csv',
    '../serpapi_results_filtered/AP_sintomas_dengue.csv',
    '../serpapi_results_filtered/AP_dengue.csv',
    '../serpapi_results_filtered/CE_sintomas_dengue.csv',
    '../serpapi_results_filtered/CE_dengue.csv',
    '../serpapi_results_filtered/DF_mosquito.csv',
    '../serpapi_results_filtered/DF_sintomas_dengue.csv',
    '../serpapi_results_filtered/DF_dengue.csv',
    '../serpapi_results_filtered/RO_mosquito.csv',
    '../serpapi_results_filtered/RO_sintomas_dengue.csv',
    '../serpapi_results_filtered/RO_dengue.csv',
    '../serpapi_results_filtered/RR_mosquito.csv',
    '../serpapi_results_filtered/RR_sintomas_dengue.csv',
    '../serpapi_results_filtered/RR_dengue.csv',
    '../serpapi_results_filtered/PA_mosquito.csv',
    '../serpapi_results_filtered/PA_sintomas_dengue.csv',
    '../serpapi_results_filtered/PA_dengue.csv',
    '../serpapi_results_filtered/GO_mosquito.csv',
    '../serpapi_results_filtered/GO_sintomas_dengue.csv',
    '../serpapi_results_filtered/GO_dengue.csv',
    '../serpapi_results_filtered/ES_mosquito.csv',
    '../serpapi_results_filtered/ES_sintomas_dengue.csv',
    '../serpapi_results_filtered/ES_dengue.csv',
    '../serpapi_results_filtered/TO_mosquito.csv',
    '../serpapi_results_filtered/TO_sintomas_dengue.csv',
    '../serpapi_results_filtered/TO_dengue.csv',
    '../serpapi_results_filtered/MA_mosquito.csv',
    '../serpapi_results_filtered/MA_sintomas_dengue.csv',
    '../serpapi_results_filtered/MA_dengue.csv',
    '../serpapi_results_filtered/PI_mosquito.csv',
    '../serpapi_results_filtered/PI_sintomas_dengue.csv',
    '../serpapi_results_filtered/PI_dengue.csv',
    '../serpapi_results_filtered/RN_mosquito.csv',
    '../serpapi_results_filtered/RN_sintomas_dengue.csv',
    '../serpapi_results_filtered/RN_dengue.csv',
    '../serpapi_results_filtered/PB_mosquito.csv',
    '../serpapi_results_filtered/PB_sintomas_dengue.csv',
    '../serpapi_results_filtered/PB_dengue.csv',
    '../serpapi_results_filtered/PE_mosquito.csv',
    '../serpapi_results_filtered/PE_sintomas_dengue.csv',
    '../serpapi_results_filtered/PE_dengue.csv',
    '../serpapi_results_filtered/MG_mosquito.csv',
    '../serpapi_results_filtered/MG_sintomas_dengue.csv',
    '../serpapi_results_filtered/MG_dengue.csv',
    '../serpapi_results_filtered/SE_mosquito.csv',
    '../serpapi_results_filtered/SE_sintomas_dengue.csv',
    '../serpapi_results_filtered/SE_dengue.csv',
    '../serpapi_results_filtered/MS_mosquito.csv',
    '../serpapi_results_filtered/MS_sintomas_dengue.csv',
    '../serpapi_results_filtered/MS_dengue.csv',
    '../serpapi_results_filtered/MT_mosquito.csv',
    '../serpapi_results_filtered/MT_sintomas_dengue.csv',
    '../serpapi_results_filtered/MT_dengue.csv',
    '../serpapi_results_filtered/PR_mosquito.csv',
    '../serpapi_results_filtered/PR_sintomas_dengue.csv',
    '../serpapi_results_filtered/PR_dengue.csv',
    '../serpapi_results_filtered/RJ_mosquito.csv',
    '../serpapi_results_filtered/RJ_sintomas_dengue.csv',
    '../serpapi_results_filtered/RJ_dengue.csv',
    '../serpapi_results_filtered/RS_mosquito.csv',
    '../serpapi_results_filtered/RS_sintomas_dengue.csv',
    '../serpapi_results_filtered/RS_dengue.csv',
    '../serpapi_results_filtered/SC_mosquito.csv',
    '../serpapi_results_filtered/SC_sintomas_dengue.csv',
    '../serpapi_results_filtered/SC_dengue.csv',
    '../serpapi_results_filtered/SP_mosquito.csv',
    '../serpapi_results_filtered/SP_sintomas_dengue.csv',
    '../serpapi_results_filtered/SP_dengue.csv'
]

# # Initialize trend columns in the main dataset
# search_terms = ['mosquito', 'sintomas_dengue', 'dengue']
# for term in search_terms:
#     main_data[f'{term}_interest'] = pd.NA

# Define the required date range
start_date = '2004-01-01'
end_date = '2019-12-01'
date_range = pd.date_range(start=start_date, end=end_date, freq='MS')

# Function to impute missing values
def impute_missing(data, method='linear'):
    if method == 'linear':
        return data.interpolate(method='linear', limit_direction='forward', axis=0)
    elif method == 'mean':
        return data.fillna(data.mean())
    elif method == 'zero':
        return data.fillna(0)
    else:
        raise ValueError("Unsupported imputation method")

# Function to scale data using Min-Max Scaling
def scale_data(data):
    scaler = MinMaxScaler()
    scaled_data = scaler.fit_transform(data.values.reshape(-1, 1)).flatten()
    return pd.Series(scaled_data, index=data.index)

# Initialize trend columns in the main dataset
search_terms = ['mosquito', 'sintomas_dengue', 'dengue']
for term in search_terms:
    main_data[f'{term}_interest'] = pd.NA

# Iterate over the trend files and process data
for file_name in trends_files:
    base_name = os.path.basename(file_name)
    state_code, search_term = base_name.split('_')[0], '_'.join(base_name.split('_')[1:]).replace('.csv', '')
    dep_id = state_code_to_dep_id.get(state_code)
   
    if not dep_id:
        print(f"Skipping file {file_name} (state code {state_code} not found)")
        continue

    trends_data = pd.read_csv(file_name)
    trends_data['Date'] = pd.to_datetime(trends_data['Date'], format='%Y-%m')
    trends_data = trends_data.set_index('Date')

    # Create a full DataFrame with the complete date range
    trends_data = trends_data.reindex(date_range)
    interest_col = trends_data['Interest']

    # Check for missing data and impute if necessary
    if interest_col.isnull().any():
        interest_col = impute_missing(interest_col, method='linear')

    # Scale the values
    interest_col = scale_data(interest_col)

    # Rename and add to trends_data
    trends_data[f'{search_term}_interest'] = interest_col
    trends_data = trends_data.reset_index().rename(columns={'index': 'Date'})

    # Merge trends data with the main dataset
    main_data = pd.merge(
        main_data,
        trends_data[['Date', f'{search_term}_interest']],
        on='Date',
        how='left',
        suffixes=('', f'_{state_code}')
    )

    main_data.loc[main_data['dep_id'] == dep_id, f'{search_term}_interest'] = main_data[f'{search_term}_interest_{state_code}']
    main_data.drop(columns=[f'{search_term}_interest_{state_code}'], inplace=True)

# Save the final merged dataset
main_data.Month = MinMaxScaler().fit_transform(main_data.Month.values.reshape(-1, 1))


main_data.drop('Date', axis=1, inplace=True)
    
main_data.to_csv('merged_dataset.csv', index=False)
print("Merged dataset saved as 'merged_dataset.csv'")

# main_data.columns


main_data=pd.read_csv('merged_dataset.csv')
main_data.columns


# Move specified columns to the end
columns_to_move = ['DengRate_all', 'DengRate_019']
main_data = main_data[[col for col in main_data.columns if col not in columns_to_move] + columns_to_move]

# Save the updated DataFrame
main_data.to_csv('merged_dataset.csv', index=False)

print("Updated dataset saved as 'merged_dataset_updated.csv'")




