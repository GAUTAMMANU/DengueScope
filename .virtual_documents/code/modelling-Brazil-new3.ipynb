





# !git clone https://github.com/GAUTAMMANU/projminor.git
# %cd projminor


# %cd code
# insert your desired path to work on
import os
from os.path import join
project_path = os.path.dirname(os.getcwd())
# os.chdir(join('..','data'))
os.getcwd()





import sys
sys.path.append(join(project_path, 'code'))





%load_ext autoreload
%autoreload 2





import matplotlib
font = {'family':'Arial', 'size':'15', 'weight':'normal'}

matplotlib.rc('font', **font)





config = {
    'main_brazil': 'Brazil',
    'main_peru': 'Peru',
    'baseline': join(project_path, "baseline_models"),
    'output': join(project_path, "code", "saved_models"),
    'metrics': join(project_path, "code", "metrics")
}
project_path

# List comprehension for the folder structure code
[os.makedirs(val, exist_ok=True) for key, val in config.items()]





import utils
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import scipy.stats as st
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
from datetime import datetime
from glob import glob
from config import DEP_NAMES, GROUPED_VARS, DATA_REDUCER_SETTINGS, DATA_PROCESSING_SETTINGS









dataframe = pd.read_csv(join('dataset', "Brazil_UF_dengue_monthly.csv"))
dataframe.head()
dataframe.iloc[1000]





cnn = pd.read_csv(join('saved_models', "cnn_dataframe.csv")).drop('Unnamed: 0', axis=1)
cnn['CD_UF'] = cnn['CD_UF'].astype(np.int64)

assert dataframe.shape[0] == cnn.shape[0]
assert all(dataframe['CD_UF'].unique() == cnn['CD_UF'].unique())
cnn


dataframe.sort_values(['CD_UF', 'Date'], inplace=True, ignore_index=True)
dataframe


dataframe = pd.concat([dataframe, cnn[['CNN_all', 'CNN_0-19']]], axis=1)
dataframe





dataframe = utils.clean(dataframe)
dataframe.head()


dataframe.info()


dataframe.to_csv('out.csv',index=False)





print('\033[1m PCA Excluded Variables \033[0m')
utils.plist(GROUPED_VARS['EXCLUDED'])

print('\033[1m Climatic variables \033[0m')
utils.plist(GROUPED_VARS['CLIMATIC VARIABLES'])

print('\033[1m Geo variables \033[0m')
utils.plist(GROUPED_VARS['GEO VARIABLES'])

print('\033[1m Socio variables \033[0m')
utils.plist(GROUPED_VARS['SOCIO VARIABLES'])

print('\033[1m Additional variables \033[0m')
utils.plist(GROUPED_VARS['AUXILIAR'])

print('\033[1m Dengue variables \033[0m')
utils.plist(GROUPED_VARS['DENGUE'])





from data_reduction import pca_reducer, pls_reducer





X_climatic = dataframe[GROUPED_VARS['CLIMATIC VARIABLES']].values
X_geo = dataframe[GROUPED_VARS['GEO VARIABLES']].values
X_socio = dataframe[GROUPED_VARS['SOCIO VARIABLES']].values









y_dengue = dataframe[GROUPED_VARS['DENGUE']].values
scaler = MinMaxScaler()
y_dengue = scaler.fit_transform(y_dengue)





if DATA_REDUCER_SETTINGS['TYPE'] == 'PLS':
    climatic_vars_reduced = pls_reducer(
        X_climatic,
        y_dengue,
        DATA_REDUCER_SETTINGS['NUMBER OF COMPONENTS']['CLIMATIC VARIABLES'])

    geo_vars_reduced = pls_reducer(
        X_geo,
        y_dengue,
        DATA_REDUCER_SETTINGS['NUMBER OF COMPONENTS']['GEO VARIABLES'])

    socio_vars_reduced = pls_reducer(
        X_socio,
        y_dengue,
        DATA_REDUCER_SETTINGS['NUMBER OF COMPONENTS']['SOCIO VARIABLES'])
    print(socio_vars_reduced.shape    )
elif DATA_REDUCER_SETTINGS['TYPE'] == 'PCA':
    climatic_vars_reduced = pca_reducer(
        X_climatic,
        DATA_REDUCER_SETTINGS['NUMBER OF COMPONENTS']['CLIMATIC VARIABLES'])

    geo_vars_reduced = pca_reducer(
        X_geo,
        DATA_REDUCER_SETTINGS['NUMBER OF COMPONENTS']['GEO VARIABLES'])

    socio_vars_reduced = pca_reducer(
        X_socio,
        DATA_REDUCER_SETTINGS['NUMBER OF COMPONENTS']['SOCIO VARIABLES'])
else:
    print('No data reduction.')








x_excluded = dataframe[GROUPED_VARS['EXCLUDED']].values
x_excluded = MinMaxScaler().fit_transform(x_excluded)


X_auxiliar = dataframe[GROUPED_VARS['AUXILIAR']].values
X_auxiliar = MinMaxScaler().fit_transform(X_auxiliar)





independent = {'Year':dataframe.Year.values, 'dep_id':dataframe.CD_UF.values, 't_fundc_ocup18m':x_excluded[:, 0], 't_medioc_ocup18m':x_excluded[:, 1],
               'PopTotal_Urban_UF':x_excluded[:, 2], 'PopTotal_Rural_UF':x_excluded[:, 3], 'total_precipitation_d':x_excluded[:, 4],
               'surface_pressure_d':x_excluded[:, 5], 'area_km2':x_excluded[:, 6], 'humidity_d':x_excluded[:, 7], 'temperature_2m_d':x_excluded[:, 8],
               'min_temperature_2m_d':x_excluded[:, 9], 'CNN_all':x_excluded[:, 10], 'CNN_0-19':x_excluded[:, 11]}

auxiliar    = {'Month': X_auxiliar[:, 0],
               'cases20_99': X_auxiliar[:, 1], 'cases0_19': X_auxiliar[:, 2],
               'RandEffects1':  MinMaxScaler().fit_transform(np.reshape(dataframe.CD_UF.values*dataframe.Month.values, (dataframe.CD_UF.values.shape[0], 1)))[:,0],
               'RandEffects2':  MinMaxScaler().fit_transform(np.reshape(dataframe.CD_UF.values*dataframe.Year.values, (dataframe.CD_UF.values.shape[0], 1)))[:,0],
               'RandEffects3':  MinMaxScaler().fit_transform(np.reshape(dataframe.CD_UF.values*dataframe.Month.values*dataframe.Year.values, (dataframe.CD_UF.values.shape[0], 1)))[:,0]}

climatic    = {'PCA0-Climatic':climatic_vars_reduced[:,0], 'PCA1-Climatic':climatic_vars_reduced[:,1], 'PCA2-Climatic':climatic_vars_reduced[:,2],
               'PCA3-Climatic':climatic_vars_reduced[:,3]}

geo         = {'PCA0-Geo':geo_vars_reduced[:,0], 'PCA1-Geo':geo_vars_reduced[:,1], 'PCA2-Geo':geo_vars_reduced[:,2],
               'PCA3-Geo':geo_vars_reduced[:,3], 'PCA4-Geo':geo_vars_reduced[:,4], 'PCA5-Geo':geo_vars_reduced[:,5]}

socio       = {'PCA0-Socio':socio_vars_reduced[:,0], 'PCA1-Socio':socio_vars_reduced[:,1], 'PCA2-Socio':socio_vars_reduced[:,2],
               'PCA3-Socio':socio_vars_reduced[:,3], 'PCA4-Socio':socio_vars_reduced[:,4], 'PCA5-Socio':socio_vars_reduced[:,5]}

dengue      = {'DengRate_all': y_dengue[:,0], 'DengRate_019': y_dengue[:,1]}

columns     = {**independent, **auxiliar, **climatic, **geo, **socio, **dengue}

reduced_dataframe = pd.DataFrame(columns)
reduced_dataframe.head()
reduced_dataframe.columns

# reduced_dataframe


reduced_dataframe=reduced_dataframe[reduced_dataframe['Year']>=2004]
reduced_dataframe.to_csv('reduced_2004_2019.csv', index=False)
reduced_dataframe.shape


# reduced_dataframe=pd.read_csv('reduced_2004_2019.csv')





reduced_dataframe=pd.read_csv('../google trends/merged_dataset.csv')
reduced_dataframe=pd.read_csv('../google trends/merged_dataset_lagged.csv')
print(reduced_dataframe.shape)
print(reduced_dataframe.columns)


training_dataframe = reduced_dataframe[reduced_dataframe.Year <= 2017]
validation_dataframe = reduced_dataframe[reduced_dataframe.Year >= 2017]
print(len(training_dataframe),
len(validation_dataframe))
training_dataframe.head()
validation_dataframe.head()





from datasetHandler import datasetHandler
dataset_handler = datasetHandler(training_dataframe, validation_dataframe)





# x_train, y_train, x_val, y_val = dataset_handler.get_data(DATA_PROCESSING_SETTINGS['T LEARNING'], DATA_PROCESSING_SETTINGS['T PREDICTION'])

x_train, y_train, x_val, y_val, train_indices, val_indices = dataset_handler.get_data(
    DATA_PROCESSING_SETTINGS['T LEARNING'], DATA_PROCESSING_SETTINGS['T PREDICTION']
)

print('\n\nX Training shape', x_train.shape)
print('Y Training shape', y_train.shape)
print('X Validation shape', x_val.shape)
print('Y Validation shape', y_val.shape)






def augment(self, x_train, y_train, x_val, y_val, augmentation_factor, static_train=None, static_val=None):
    # Augment x_train and y_train as usual
    x_train_augmented = np.tile(x_train, (augmentation_factor, 1, 1))
    y_train_augmented = np.tile(y_train, (augmentation_factor, 1))
    x_val_augmented = np.tile(x_val, (augmentation_factor, 1, 1))
    y_val_augmented = np.tile(y_val, (augmentation_factor, 1))
    
    # Augment static data if provided
    if static_train is not None:
        static_train_augmented = np.tile(static_train, (augmentation_factor, 1))
    else:
        static_train_augmented = None

    if static_val is not None:
        static_val_augmented = np.tile(static_val, (augmentation_factor, 1))
    else:
        static_val_augmented = None

    return x_train_augmented, y_train_augmented, x_val_augmented, y_val_augmented, static_train_augmented, static_val_augmented



x_train_a, y_train_a, x_val_a, y_val_a = dataset_handler.augment(x_train, y_train, x_val, y_val, DATA_PROCESSING_SETTINGS['AUGMENTATION'])
print('X Training shape', x_train_a.shape)
print('Y Training shape', y_train_a.shape)
print('X Validation shape', x_val_a.shape)
print('Y Validation shape', y_val_a.shape)

# # Augment static data to match temporal data
# static_train_a = np.repeat(static_train, repeats=DATA_PROCESSING_SETTINGS['AUGMENTATION'], axis=0)
# static_val_a = np.repeat(static_val, repeats=DATA_PROCESSING_SETTINGS['AUGMENTATION'], axis=0)

# # Validate shapes
# # Validate shapes
# print(f"x_train_a shape: {x_train_a.shape}, static_train_a shape: {static_train_a.shape}")
# print(f"x_val_a shape: {x_val_a.shape}, static_val_a shape: {static_val_a.shape}")
# assert x_train_a.shape[0] == static_train_a.shape[0], "Mismatch after augmentation: x_train_a and static_train_a"
# assert x_val_a.shape[0] == static_val_a.shape[0], "Mismatch after augmentation: x_val_a and static_val_a"

# x_train_a, y_train_a, x_val_a, y_val_a, static_train_a, static_val_a = dataset_handler.augment(
#     x_train, y_train, x_val, y_val, 
#     DATA_PROCESSING_SETTINGS['AUGMENTATION'], 
#     static_train=static_train, 
#     static_val=static_val
# )

# # Validate shapes
# print(f"x_train_a shape: {x_train_a.shape}, static_train_a shape: {static_train_a.shape}")
# print(f"x_val_a shape: {x_val_a.shape}, static_val_a shape: {static_val_a.shape}")
# assert x_train_a.shape[0] == static_train_a.shape[0], "Mismatch after augmentation: x_train_a and static_train_a"
# assert x_val_a.shape[0] == static_val_a.shape[0], "Mismatch after augmentation: x_val_a and static_val_a"





from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, Dense, Flatten, BatchNormalization, Dropout, Input, Activation
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.models import load_model
from sklearn.metrics import root_mean_squared_error
from keras.metrics import MeanSquaredError, MeanAbsoluteError
from keras_tuner import HyperModel, RandomSearch
from tensorflow.keras.optimizers import Adam
import tensorflow as tf
from datetime import datetime
from glob import glob
from tensorflow.keras.models import load_model
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error



from tensorflow.keras import layers, models, regularizers, Input
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers.schedules import ExponentialDecay

LSTM_SETTINGS = {
    'EPOCHS': 200,
    'LEARNING RATE': 0.0001,
    'BATCH SIZE': 16,
    'OPTIMZER': 'rmsprop', #'adam',
    'LOSS':'mae',
    'EVALUATION METRIC':['mse'],
    'EARLY STOPPING': 12
}

def build_tcn_model_v2(input_shape, output_units):
    def residual_block(x, filters, dilation_rate):
        shortcut = x  # Residual connection
        x = layers.Conv1D(filters, kernel_size=3, padding='causal', 
                          activation='relu', dilation_rate=dilation_rate,
                          kernel_regularizer=regularizers.l2(1e-4))(x)
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(0.3)(x)
        
        x = layers.Conv1D(filters, kernel_size=3, padding='causal', 
                          activation='relu', dilation_rate=dilation_rate,
                          kernel_regularizer=regularizers.l2(1e-4))(x)
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(0.3)(x)
        
        # Add residual connection
        x = layers.add([x, shortcut])
        x = layers.ReLU()(x)
        return x

    inputs = Input(shape=input_shape)
    x = layers.Conv1D(filters=64, kernel_size=3, padding='causal', activation='relu')(inputs)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.2)(x)
    
    # Add residual blocks with increasing dilation rates
    for dilation_rate in [1, 2, 4, 8]:
        x = residual_block(x, filters=64, dilation_rate=dilation_rate)
    
    x = layers.Flatten()(x)
    outputs = layers.Dense(units=output_units, activation='linear')(x)  # For regression
    
    model = models.Model(inputs, outputs)
    model.compile(optimizer='adam', loss='Huber', metrics=['mae'])
    return model

class ImprovedTCNNet:
    def __init__(self, shape, output_units=2):
        self.shape = shape
        self.epochs = LSTM_SETTINGS['EPOCHS']  # Max epochs
        self.batch_size = LSTM_SETTINGS['BATCH SIZE']
        self.lr = LSTM_SETTINGS['LEARNING RATE']
        self.early_stopping_rounds = LSTM_SETTINGS['EARLY STOPPING']
        
        self.model = build_tcn_model_v2(self.shape, output_units)
    
    def load(self, model_path):
            """Load a saved model from the specified path."""
            self.model = tf.keras.models.load_model(model_path)
            print(f"Model loaded successfully from {model_path}")
        
    def train(self, training, validation, output_path):
        # Early stopping and learning rate scheduler
        es = EarlyStopping(
            monitor='val_loss',
            patience=self.early_stopping_rounds,
            restore_best_weights=True
        )
        
        lr_scheduler = ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=5,
            min_lr=1e-6,
            verbose=1
        )
        
        # Train the model
        history = self.model.fit(
            x=training[0],
            y=training[1],
            validation_data=(validation[0], validation[1]),
            epochs=self.epochs,
            batch_size=self.batch_size,
            callbacks=[es, lr_scheduler],
            shuffle=True
        )

        # Plot the training history
        plt.figure(figsize=(8, 6))
        plt.plot(history.history['loss'], label='Train Loss')
        plt.plot(history.history['val_loss'], label='Validation Loss')
        plt.legend()
        plt.title('Improved Model Loss Curve')
        plt.xlabel('Epochs')
        plt.ylabel('Loss')
        plt.show()

        # Save the model
        today = datetime.now().strftime("%d-%m-%Y-%H-%M-%S")
        self.model.save(os.path.join(output_path, 'TCN-new-search-' + today + '.keras'))
        return history

# Use the improved model
trainingT, validationT = dataset_handler.prepare_data_LSTM(x_train_a[:,:,2:], y_train_a, x_val_a[:,:,2:], y_val_a)
tcn = ImprovedTCNNet(trainingT[0].shape[1:])




# Define paths
output_path = os.path.join(config['output'], "Brazil")
os.makedirs(output_path, exist_ok=True)

# Define training flag
TRAINING = False  # Set to False to skip training and load the model if it exists


# Check if training is required
if TRAINING:
    print("Training the model...")

    # history = tcn.train(trainingT, validationT, output_path=join(config['output'], "Brazil"))
    # Train the model with augmented data
    history = tcn.train(
        training=trainingT,
        validation=validationT,
        output_path=join(config['output'], "Brazil")
    )


else:
    # Check if a saved model exists
    print("Checking for saved models...")
    tcn_models = glob(os.path.join(output_path, "TCN-new-search-*.keras"))
    if not tcn_models:
        print('No file with such pattern was found in the directory. Run TRAINING = True first.')
        exit()
    else:
        tcn.load(tcn_models[-1])
        print(f"Loading model from: {tcn_models[-1]}")
    
    trainT, valT = dataset_handler.prepare_data_LSTM(x_train[:,:,2:], y_train, x_val[:,:,2:], y_val)
    
    # Create index mappings
    y_val_indices_df = pd.DataFrame(val_indices, columns=['actual_index'])
    y_train_indices_df = pd.DataFrame(train_indices, columns=['actual_index'])

    # Make predictions on training and validation sets
    preds_tra = tcn.model.predict(trainT[0])
    preds_tra[preds_tra < 0] = 0
    preds_val = tcn.model.predict(valT[0])
    preds_val[preds_val < 0] = 0

    results = []
    preds_val_original = scaler.inverse_transform(preds_val)
    y_val_original = scaler.inverse_transform(valT[1])
    
    # Inverse transform predictions and ground truth values for training data
    preds_tra_original = scaler.inverse_transform(preds_tra)
    y_train_original = scaler.inverse_transform(trainT[1])

    for department_idx, department_name in DEP_NAMES.items():
        # Filter rows corresponding to the current department for both training and validation
        department_rows_val = validation_dataframe[validation_dataframe['dep_id'] == department_idx]
        department_rows_train = training_dataframe[training_dataframe['dep_id'] == department_idx]
    
        if department_rows_val.empty or department_rows_train.empty:
            continue
    
        department_indices_val = department_rows_val.index.tolist()
        department_indices_train = department_rows_train.index.tolist()

        # Matching indices for validation data
        matching_indices_val = y_val_indices_df[y_val_indices_df['actual_index'].isin(department_indices_val)].index
        # Matching indices for training data
        matching_indices_train = y_train_indices_df[y_train_indices_df['actual_index'].isin(department_indices_train)].index
        
        if matching_indices_val.empty or matching_indices_train.empty:
            continue

        # Extract original scale true and predicted values for validation data
        true_dengrate_all_val = y_val_original[matching_indices_val, 0]
        true_dengrate_019_val = y_val_original[matching_indices_val, 1]
        predicted_dengrate_all_val = preds_val_original[matching_indices_val, 0]
        predicted_dengrate_019_val = preds_val_original[matching_indices_val, 1]
    
        # Extract original scale true and predicted values for training data
        true_dengrate_all_train = y_train_original[matching_indices_train, 0]
        true_dengrate_019_train = y_train_original[matching_indices_train, 1]
        predicted_dengrate_all_train = preds_tra_original[matching_indices_train, 0]
        predicted_dengrate_019_train = preds_tra_original[matching_indices_train, 1]
    
        # Calculate NRMSE for both DengRate_all and DengRate_019 (validation data)
        nrmse_dengrate_all_val = root_mean_squared_error(true_dengrate_all_val, predicted_dengrate_all_val) / (true_dengrate_all_val.max() - true_dengrate_all_val.min())
        nrmse_dengrate_019_val = root_mean_squared_error(true_dengrate_019_val, predicted_dengrate_019_val) / (true_dengrate_019_val.max() - true_dengrate_019_val.min())
    
        # Calculate NRMSE for both DengRate_all and DengRate_019 (training data)
        nrmse_dengrate_all_train = root_mean_squared_error(true_dengrate_all_train, predicted_dengrate_all_train) / (true_dengrate_all_train.max() - true_dengrate_all_train.min())
        nrmse_dengrate_019_train = root_mean_squared_error(true_dengrate_019_train, predicted_dengrate_019_train) / (true_dengrate_019_train.max() - true_dengrate_019_train.min())
    
        # Calculate MAE for both DengRate_all and DengRate_019 (validation data)
        mae_dengrate_all_val = mean_absolute_error(true_dengrate_all_val, predicted_dengrate_all_val)
        mae_dengrate_019_val = mean_absolute_error(true_dengrate_019_val, predicted_dengrate_019_val)
    
        # Store the results
        results.append({
            'Department': department_name,
            'NRMSE 0-19 Training': nrmse_dengrate_019_train,
            'NRMSE All Training': nrmse_dengrate_all_train,
            'NRMSE 0-19 Validation': nrmse_dengrate_019_val,
            'NRMSE All Validation': nrmse_dengrate_all_val,
            'MAE (DengRate_all) Val': mae_dengrate_all_val,
            'MAE (DengRate_019) Val': mae_dengrate_019_val
        })


    # print(results)
    results_df = pd.DataFrame(results)

    #Save results
    today = datetime.now().strftime("%d-%m-%Y-%H-%M-%S")
    output_path = os.path.join(config['metrics'], "Brazil", f'TCN_model_search_lagged_{today}.csv')

    # Create the directory if it doesn't exist
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    results_df.to_csv(output_path, index=False)
    print(f"Results saved to {output_path}")





from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Input, Flatten
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import load_model
from keras.metrics import MeanSquaredError, MeanAbsoluteError
from datetime import datetime
import matplotlib.pyplot as plt


trainingL, validationL = dataset_handler.prepare_data_LSTM(x_train_a[:,:,2:], y_train_a, x_val_a[:,:,2:], y_val_a)


LSTM_SETTINGS = {
    'EPOCHS': 200,
    'LEARNING RATE': 0.0001,
    'BATCH SIZE': 16,
    'OPTIMZER': 'rmsprop', #'adam',
    'LOSS':'mae',
    'EVALUATION METRIC':['mse'],
    'EARLY STOPPING': 12
}

def custom_load_model(path):
    return load_model(
        path,
        custom_objects={
            'mae': MeanAbsoluteError(),
            'mse': MeanSquaredError(),
        },
        compile=True
    )
    
class LSTMNet:
    def __init__(self, shape):
        self.shape = shape
        self.epochs = LSTM_SETTINGS['EPOCHS']
        self.lr = LSTM_SETTINGS['LEARNING RATE']
        self.batch_size = LSTM_SETTINGS['BATCH SIZE']
        self.loss = LSTM_SETTINGS['LOSS']
        self.eval_metric = LSTM_SETTINGS['EVALUATION METRIC']
        self.early_stopping_rounds = LSTM_SETTINGS['EARLY STOPPING']
        
        # Use .get() to avoid KeyError if 'OPTIMIZER' is missing
        self.optimizer = LSTM_SETTINGS.get('OPTIMIZER', 'adam')
        if self.optimizer == 'adam':
            self.optimizer = Adam(learning_rate=self.lr)
        elif self.optimizer == 'rmsprop':
            self.optimizer = RMSprop(learning_rate=self.lr)
        else:
            self.optimizer = Adam(learning_rate=self.lr)

        self.model = self.__build()

    def __build(self):
        model = Sequential()
        model.add(Input(shape=self.shape))
        model.add(LSTM(60, return_sequences=True, dropout=0.5))
        model.add(LSTM(20, dropout=0.5))
        model.add(Dense(2))
        model.compile(loss=self.loss, metrics=self.eval_metric, optimizer=self.optimizer)
        return model

    # def load(self, path):
    #     self.model = load_model(path)
    def load(self, path):
        self.model = custom_load_model(path)

    def train(self, training, validation, output_path):
        es = EarlyStopping(
            monitor='val_loss', 
            min_delta=0, 
            patience=self.early_stopping_rounds, 
            verbose=0, 
            mode='auto', 
            baseline=None, 
            restore_best_weights=True
        )

        history = self.model.fit(
            x=training[0],
            y=training[1],
            validation_data=(validation[0], validation[1]),
            epochs=self.epochs,
            batch_size=self.batch_size,
            callbacks=[es],
            shuffle=True
        )

            # Plot the training history
        plt.figure(figsize=(8, 6))
        plt.plot(history.history['loss'], label='Train Loss')
        plt.plot(history.history['val_loss'], label='Validation Loss')
        plt.legend()
        plt.title('Model Loss Curve')
        plt.xlabel('Epochs')
        plt.ylabel('Loss')
        plt.show()

        today = datetime.now().strftime("%d-%m-%Y-%H-%M-%S")
        self.model.save(os.path.join(output_path, 'LSTM-search-' + today + '.h5'))
        return history

lstm = LSTMNet(trainingL[0].shape[1:])



TRAINING = False

if TRAINING:
    lstm.train(trainingL, validationL, output_path=join(config['output'], "Brazil"))

else:
    # get most recent lstm model
    lstm_model = glob(join(config['output'], "Brazil", "LSTM-search-*"))
    if not lstm_model:
        print('No file with such pattern was found in the directory. Run TRAINING = True first.')
        exit()
    else:
        lstm.load(lstm_model[-1])
        print(f"Loading model from: {lstm_model[-1]}")

    trainL, valL = dataset_handler.prepare_data_LSTM(x_train[:,:,2:], y_train, x_val[:,:,2:], y_val)
    
    preds_tra = lstm.model.predict(trainL[0])
    preds_tra[preds_tra < 0] = 0
    preds_val = lstm.model.predict(valL[0])
    preds_val[preds_val < 0] = 0
    # print("preds_tra: \n",preds_tra.shape,"\n")
    # print("preds_val: \n",preds_val.shape,"\n")
    
        # Align predictions with department indices
    # y_val_indices = validation_dataframe.index.to_list()
    # y_val_indices_df = pd.DataFrame(y_val_indices, columns=['actual_index'])
    
    # y_train_indices = training_dataframe.index.to_list()
    # y_train_indices_df = pd.DataFrame(y_train_indices, columns=['actual_index'])

    # Create index mappings
    y_val_indices_df = pd.DataFrame(val_indices, columns=['actual_index'])
    y_train_indices_df = pd.DataFrame(train_indices, columns=['actual_index'])
    
    # Initialize results list
    results = []
    preds_val_original = scaler.inverse_transform(preds_val)
    y_val_original = scaler.inverse_transform(valL[1])
    
    # Inverse transform predictions and ground truth values for training data
    preds_tra_original = scaler.inverse_transform(preds_tra)
    y_train_original = scaler.inverse_transform(trainL[1])
    
    for department_idx, department_name in DEP_NAMES.items():
        # Filter rows corresponding to the current department for both training and validation
        department_rows_val = validation_dataframe[validation_dataframe['dep_id'] == department_idx]
        department_rows_train = training_dataframe[training_dataframe['dep_id'] == department_idx]
    
        if department_rows_val.empty or department_rows_train.empty:
            continue
    
        department_indices_val = department_rows_val.index.tolist()
        department_indices_train = department_rows_train.index.tolist()
    
        # # Matching indices for validation data
        # matching_indices_val = y_val_indices_df[y_val_indices_df['actual_index'].isin(department_indices_val)].index
        # matching_indices_train = y_train_indices_df[y_train_indices_df['actual_index'].isin(department_indices_train)].index

        # Matching indices for validation data
        matching_indices_val = y_val_indices_df[y_val_indices_df['actual_index'].isin(department_indices_val)].index
        # Matching indices for training data
        matching_indices_train = y_train_indices_df[y_train_indices_df['actual_index'].isin(department_indices_train)].index
        
        if matching_indices_val.empty or matching_indices_train.empty:
            continue
    
        # Extract original scale true and predicted values for validation data
        true_dengrate_all_val = y_val_original[matching_indices_val, 0]
        true_dengrate_019_val = y_val_original[matching_indices_val, 1]
        predicted_dengrate_all_val = preds_val_original[matching_indices_val, 0]
        predicted_dengrate_019_val = preds_val_original[matching_indices_val, 1]
    
        # Extract original scale true and predicted values for training data
        true_dengrate_all_train = y_train_original[matching_indices_train, 0]
        true_dengrate_019_train = y_train_original[matching_indices_train, 1]
        predicted_dengrate_all_train = preds_tra_original[matching_indices_train, 0]
        predicted_dengrate_019_train = preds_tra_original[matching_indices_train, 1]
    
        # Calculate NRMSE for both DengRate_all and DengRate_019 (validation data)
        nrmse_dengrate_all_val = root_mean_squared_error(true_dengrate_all_val, predicted_dengrate_all_val) / (true_dengrate_all_val.max() - true_dengrate_all_val.min())
        nrmse_dengrate_019_val = root_mean_squared_error(true_dengrate_019_val, predicted_dengrate_019_val) / (true_dengrate_019_val.max() - true_dengrate_019_val.min())
    
        # Calculate NRMSE for both DengRate_all and DengRate_019 (training data)
        nrmse_dengrate_all_train = root_mean_squared_error(true_dengrate_all_train, predicted_dengrate_all_train) / (true_dengrate_all_train.max() - true_dengrate_all_train.min())
        nrmse_dengrate_019_train = root_mean_squared_error(true_dengrate_019_train, predicted_dengrate_019_train) / (true_dengrate_019_train.max() - true_dengrate_019_train.min())
    
        # Calculate MAE for both DengRate_all and DengRate_019 (validation data)
        mae_dengrate_all_val = mean_absolute_error(true_dengrate_all_val, predicted_dengrate_all_val)
        mae_dengrate_019_val = mean_absolute_error(true_dengrate_019_val, predicted_dengrate_019_val)
    
        # Store the results
        results.append({
            'Department': department_name,
            'NRMSE 0-19 Training': nrmse_dengrate_019_train,
            'NRMSE All Training': nrmse_dengrate_all_train,
            'NRMSE 0-19 Validation': nrmse_dengrate_019_val,
            'NRMSE All Validation': nrmse_dengrate_all_val,
            'MAE (DengRate_all) Val': mae_dengrate_all_val,
            'MAE (DengRate_019) Val': mae_dengrate_019_val
        })

    # print(results)
    results_df = pd.DataFrame(results)

    #Save results
    today = datetime.now().strftime("%d-%m-%Y-%H-%M-%S")
    output_path = os.path.join(config['metrics'], "Brazil", f'LSTM_Model_search_{today}.csv')

    # Create the directory if it doesn't exist
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    results_df.to_csv(output_path, index=False)
    print(f"Results saved to {output_path}")


















from keras.metrics import MeanSquaredError, MeanAbsoluteError
from sklearn.metrics import mean_absolute_error


trainingC, validationC = dataset_handler.prepare_data_CatBoost(x_train_a[:,:,2:], y_train_a, x_val_a[:,:,2:], y_val_a)





from models import CatBoostNet
cat_boost = CatBoostNet()


cat_boost.model.get_params()





def compute_scores(gt, preds):
    # Compute Normalized-RMSE and R2 metrics
    rmse = root_mean_squared_error(gt, preds)
    nrmse = rmse/(gt.max() - gt.min())
    r2 = r2_score(gt, preds)
    return nrmse, r2


TRAINING = False
# nb_deps = 27

if TRAINING:
    # cat_boost.train(trainingC, validationC, output_path=join(config['output'], "Brazil"))

    snapshot_path = os.path.join(config['output'], "Brazil", f"snapshot-{today}.bkp")
    cat_boost.train(
        trainingC,
        validationC,
        output_path=os.path.join(config['output'], "Brazil", f"CATBOOST-search-{today}"),
        snapshot_file=snapshot_path,
        snapshot_interval=300  # Use a new snapshot
    )

else:
    # get most recent catboost model
    cat_boost_model = glob(join(config['output'], "Brazil", "CATBOOST-search-*"))
    if not cat_boost_model:
        print('No file with such pattern was found in the directory. Run TRAINING = True first.')
        exit()
    else:
        cat_boost.load(cat_boost_model[-1])
        print(f"Loading model from: {cat_boost_model[-1]}")

    trainC, valC = dataset_handler.prepare_data_CatBoost(x_train[:,:,2:], y_train, x_val[:,:,2:], y_val)
    
    preds_tra = cat_boost.model.predict(trainC[0])
    preds_tra[preds_tra < 0] = 0
    preds_val = cat_boost.model.predict(valC[0])
    preds_val[preds_val < 0] = 0
    # print("preds_tra: \n",preds_tra.shape,"\n")
    # print("preds_val: \n",preds_val.shape,"\n")
    
        # Align predictions with department indices
    # y_val_indices = validation_dataframe.index.to_list()
    # y_val_indices_df = pd.DataFrame(y_val_indices, columns=['actual_index'])
    
    # y_train_indices = training_dataframe.index.to_list()
    # y_train_indices_df = pd.DataFrame(y_train_indices, columns=['actual_index'])

    # Create index mappings
    y_val_indices_df = pd.DataFrame(val_indices, columns=['actual_index'])
    y_train_indices_df = pd.DataFrame(train_indices, columns=['actual_index'])

    # Initialize results list
    results = []
    preds_val_original = scaler.inverse_transform(preds_val)
    y_val_original = scaler.inverse_transform(valL[1])
    
    # Inverse transform predictions and ground truth values for training data
    preds_tra_original = scaler.inverse_transform(preds_tra)
    y_train_original = scaler.inverse_transform(trainL[1])
    
    for department_idx, department_name in DEP_NAMES.items():
        # Filter rows corresponding to the current department for both training and validation
        department_rows_val = validation_dataframe[validation_dataframe['dep_id'] == department_idx]
        department_rows_train = training_dataframe[training_dataframe['dep_id'] == department_idx]
    
        if department_rows_val.empty or department_rows_train.empty:
            continue
    
        department_indices_val = department_rows_val.index.tolist()
        department_indices_train = department_rows_train.index.tolist()
    
        # Matching indices for validation data
        # matching_indices_val = y_val_indices_df[y_val_indices_df['actual_index'].isin(department_indices_val)].index
        # matching_indices_train = y_train_indices_df[y_train_indices_df['actual_index'].isin(department_indices_train)].index

        # Matching indices for validation data
        matching_indices_val = y_val_indices_df[y_val_indices_df['actual_index'].isin(department_indices_val)].index
        # Matching indices for training data
        matching_indices_train = y_train_indices_df[y_train_indices_df['actual_index'].isin(department_indices_train)].index
    
        if matching_indices_val.empty or matching_indices_train.empty:
            continue
    
        # Extract original scale true and predicted values for validation data
        true_dengrate_all_val = y_val_original[matching_indices_val, 0]
        true_dengrate_019_val = y_val_original[matching_indices_val, 1]
        predicted_dengrate_all_val = preds_val_original[matching_indices_val, 0]
        predicted_dengrate_019_val = preds_val_original[matching_indices_val, 1]
    
        # Extract original scale true and predicted values for training data
        true_dengrate_all_train = y_train_original[matching_indices_train, 0]
        true_dengrate_019_train = y_train_original[matching_indices_train, 1]
        predicted_dengrate_all_train = preds_tra_original[matching_indices_train, 0]
        predicted_dengrate_019_train = preds_tra_original[matching_indices_train, 1]
    
        # Calculate NRMSE for both DengRate_all and DengRate_019 (validation data)
        nrmse_dengrate_all_val = root_mean_squared_error(true_dengrate_all_val, predicted_dengrate_all_val) / (true_dengrate_all_val.max() - true_dengrate_all_val.min())
        nrmse_dengrate_019_val = root_mean_squared_error(true_dengrate_019_val, predicted_dengrate_019_val) / (true_dengrate_019_val.max() - true_dengrate_019_val.min())
    
        # Calculate NRMSE for both DengRate_all and DengRate_019 (training data)
        nrmse_dengrate_all_train = root_mean_squared_error(true_dengrate_all_train, predicted_dengrate_all_train) / (true_dengrate_all_train.max() - true_dengrate_all_train.min())
        nrmse_dengrate_019_train = root_mean_squared_error(true_dengrate_019_train, predicted_dengrate_019_train) / (true_dengrate_019_train.max() - true_dengrate_019_train.min())
    
        # Calculate MAE for both DengRate_all and DengRate_019 (validation data)
        mae_dengrate_all_val = mean_absolute_error(true_dengrate_all_val, predicted_dengrate_all_val)
        mae_dengrate_019_val = mean_absolute_error(true_dengrate_019_val, predicted_dengrate_019_val)
    
        # Store the results
        results.append({
            'Department': department_name,
            'NRMSE 0-19 Training': nrmse_dengrate_019_train,
            'NRMSE All Training': nrmse_dengrate_all_train,
            'NRMSE 0-19 Validation': nrmse_dengrate_019_val,
            'NRMSE All Validation': nrmse_dengrate_all_val,
            'MAE (DengRate_all) Val': mae_dengrate_all_val,
            'MAE (DengRate_019) Val': mae_dengrate_019_val
        })


    # print(results)
    results_df = pd.DataFrame(results)

    #Save results
    today = datetime.now().strftime("%d-%m-%Y-%H-%M-%S")
    output_path = os.path.join(config['metrics'], "Brazil", f'catboost_search_{today}.csv')

    # Create the directory if it doesn't exist
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    results_df.to_csv(output_path, index=False)
    print(f"Results saved to {output_path}")






import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import matplotlib.pyplot as plt
import numpy as np
from datetime import datetime
import os
import pandas as pd
from sklearn.metrics import mean_absolute_error, mean_squared_error

# Dataset Class for PyTorch
class TFTDataset(Dataset):
    def __init__(self, x_data, y_data):
        self.x_data = torch.tensor(x_data, dtype=torch.float32)
        self.y_data = torch.tensor(y_data, dtype=torch.float32)

    def __len__(self):
        return len(self.x_data)

    def __getitem__(self, idx):
        return self.x_data[idx], self.y_data[idx]

# Temporal Fusion Transformer Model
class TemporalFusionTransformer(nn.Module):
    def __init__(self, input_dim, output_dim=2, hidden_size=64, num_heads=4):
        super(TemporalFusionTransformer, self).__init__()
        self.hidden_size = hidden_size

        # LSTM for time-varying input
        self.lstm = nn.LSTM(input_dim, hidden_size, batch_first=True, dropout=0.3)

        # Attention layer
        self.attention = nn.MultiheadAttention(hidden_size, num_heads, batch_first=True)
        self.layer_norm = nn.LayerNorm(hidden_size)

        # Final output layers
        self.output_dense = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_size, output_dim)
        )

    def forward(self, x):
        # LSTM processing
        lstm_out, _ = self.lstm(x)

        # Attention layer
        attn_output, _ = self.attention(lstm_out, lstm_out, lstm_out)
        attn_output = self.layer_norm(attn_output + lstm_out)  # Residual connection

        # Final output layer
        output = self.output_dense(attn_output[:, -1, :])  # Use the last time step
        return output

# Training Loop
def train_model(model, train_loader, val_loader, optimizer, criterion, device, num_epochs):
    history = {'train_loss': [], 'val_loss': []}
    for epoch in range(num_epochs):
        # Training phase
        model.train()
        train_loss = 0.0
        for x_batch, y_batch in train_loader:
            x_batch, y_batch = x_batch.to(device), y_batch.to(device)
            optimizer.zero_grad()
            outputs = model(x_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()
            train_loss += loss.item() * x_batch.size(0)

        # Validation phase
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for x_batch, y_batch in val_loader:
                x_batch, y_batch = x_batch.to(device), y_batch.to(device)
                outputs = model(x_batch)
                loss = criterion(outputs, y_batch)
                val_loss += loss.item() * x_batch.size(0)

        # Record losses
        train_loss /= len(train_loader.dataset)
        val_loss /= len(val_loader.dataset)
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        print(f"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")

    return history

# Main Execution Pipeline
def main_pipeline(x_train, y_train, x_val, y_val, model_path,num_epochs=20, training=True):
    # Hyperparameters
    input_dim = x_train.shape[2]
    output_dim = y_train.shape[1]
    hidden_size = 64
    num_heads = 4
    batch_size = 32

    # Prepare datasets and dataloaders
    train_dataset = TFTDataset(x_train, y_train)
    val_dataset = TFTDataset(x_val, y_val)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

    # Initialize model, optimizer, and loss function
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = TemporalFusionTransformer(input_dim, output_dim, hidden_size, num_heads).to(device)
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.MSELoss()

    # if training:
    #     # Train the model
    history = train_model(model, train_loader, val_loader, optimizer, criterion, device, num_epochs)

    # Plot training history
    plt.plot(history['train_loss'], label='Train Loss')
    plt.plot(history['val_loss'], label='Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss')
    plt.legend()
    plt.show()

            # Save the model
    today = datetime.now().strftime("%d-%m-%Y-%H-%M-%S")
    model_save_path = os.path.join(model_path, 'TFT_model_search_' + today + '.pt')
    torch.save(model.state_dict(), model_save_path)
    print(f"Model saved to {model_save_path}")
    return model, model_save_path



# import torch
# import torch.nn as nn
# import torch.optim as optim
# from torch.utils.data import DataLoader, Dataset
# import matplotlib.pyplot as plt
# import numpy as np
# from datetime import datetime
# import os
# import pandas as pd
# from sklearn.metrics import mean_absolute_error, mean_squared_error

# # Custom Loss Function (MSE + MAE)
# class CustomLoss(nn.Module):
#     def forward(self, preds, targets):
#         mse_loss = nn.MSELoss()(preds, targets)
#         mae_loss = nn.L1Loss()(preds, targets)
#         return mse_loss + mae_loss

# # Dataset Class for PyTorch
# class TFTDataset(Dataset):
#     def __init__(self, x_data, y_data, static_data):
#         self.x_data = torch.tensor(x_data, dtype=torch.float32)
#         self.y_data = torch.tensor(y_data, dtype=torch.float32)
#         self.static_data = torch.tensor(static_data, dtype=torch.float32)

#     def __len__(self):
#         return len(self.x_data)

#     def __getitem__(self, idx):
#         return self.x_data[idx], self.y_data[idx], self.static_data[idx]

# Improved Temporal Fusion Transformer Model
# class TemporalFusionTransformer(nn.Module):
#     def __init__(self, input_dim, static_dim, output_dim=2, hidden_size=64, num_heads=4, num_layers=2):
#         super(TemporalFusionTransformer, self).__init__()
#         self.hidden_size = hidden_size
#         self.num_layers = num_layers

#         # Static Feature Encoder
#         self.static_encoder = nn.Linear(static_dim, hidden_size)

#         # LSTM for time-varying input
        
#         self.lstm = nn.LSTM(input_dim, hidden_size, num_layers=num_layers, batch_first=True, dropout=0.3)

#         # Attention layer for temporal fusion
#         self.attention = nn.MultiheadAttention(hidden_size, num_heads, batch_first=True)
#         self.layer_norm = nn.LayerNorm(hidden_size)

#         # Variable selection network
#         self.variable_selection = nn.Sequential(
#             nn.Linear(hidden_size, hidden_size),
#             nn.ReLU(),
#             nn.Linear(hidden_size, 1),  # Output a weight for each feature
#             nn.Softmax(dim=-1)
#         )

#         # Final dense layers
#         self.output_dense = nn.Sequential(
#             nn.Linear(hidden_size, hidden_size),
#             nn.ReLU(),
#             nn.Dropout(0.3),
#             nn.Linear(hidden_size, output_dim)
#         )

#     def forward(self, x, static):
#         batch_size, time_steps, _ = x.size()

#         # Static feature processing
#         static_embedding = self.static_encoder(static)
#         static_embedding = static_embedding.unsqueeze(1).expand(-1, time_steps, -1)

#         # LSTM for temporal features
#         lstm_out, _ = self.lstm(x)

#         # Attention mechanism
#         attn_output, _ = self.attention(lstm_out, lstm_out, lstm_out)
#         attn_output = self.layer_norm(attn_output + lstm_out)  # Residual connection

#         # Variable selection
#         variable_weights = self.variable_selection(attn_output)
#         weighted_features = attn_output * variable_weights

#         # Combine static and temporal features
#         combined_features = weighted_features + static_embedding

#         # Final output layer
#         output = self.output_dense(combined_features[:, -1, :])  # Use the last time step
#         return output

# def train_model(model, train_loader, val_loader, optimizer, criterion, device, num_epochs):
#     history = {'train_loss': [], 'val_loss': []}
#     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)

#     # Initialize EarlyStopping
#     early_stopping = EarlyStopping(patience=5)

#     # Initialize variable to track best validation loss
#     best_val_loss = float('inf')
#     best_model_path = "best_model.pt"  # Path to save the best model

#     for epoch in range(num_epochs):
#         # Training phase
#         model.train()
#         train_loss = 0.0
#         for x_batch, y_batch, static_batch in train_loader:
#             x_batch, y_batch, static_batch = x_batch.to(device), y_batch.to(device), static_batch.to(device)
#             optimizer.zero_grad()
#             outputs = model(x_batch, static_batch)
#             loss = criterion(outputs, y_batch)
#             loss.backward()
#             optimizer.step()
#             train_loss += loss.item() * x_batch.size(0)

#         # Validation phase
#         model.eval()
#         val_loss = 0.0
#         with torch.no_grad():
#             for x_batch, y_batch, static_batch in val_loader:
#                 x_batch, y_batch, static_batch = x_batch.to(device), y_batch.to(device), static_batch.to(device)
#                 outputs = model(x_batch, static_batch)
#                 loss = criterion(outputs, y_batch)
#                 val_loss += loss.item() * x_batch.size(0)

#         # Average losses
#         train_loss /= len(train_loader.dataset)
#         val_loss /= len(val_loader.dataset)
#         history['train_loss'].append(train_loss)
#         history['val_loss'].append(val_loss)

#         # Save the best model
#         if val_loss < best_val_loss:
#             best_val_loss = val_loss
#             torch.save(model.state_dict(), best_model_path)
#             print(f"Best model saved at epoch {epoch+1} with val_loss={val_loss:.4f}")

#         # Learning rate scheduler
#         scheduler.step(val_loss)

#         print(f"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")

#         # Early stopping
#         early_stopping(val_loss)
#         if early_stopping.early_stop:
#             print(f"Early stopping at epoch {epoch+1}")
#             break

#     return history



# # Main Execution Pipeline
# def main_pipeline(x_train, y_train, x_val, y_val, static_train, static_val, model_path, num_epochs=20, training=True):
#     # Hyperparameters
#     # Assertions for consistency between temporal and static features
#     assert x_train.shape[0] == static_train.shape[0], "Mismatch between x_train and static_train samples"
#     assert x_val.shape[0] == static_val.shape[0], "Mismatch between x_val and static_val samples"

    
#     input_dim = x_train.shape[2]
#     static_dim = static_train.shape[1]
#     output_dim = y_train.shape[1]
#     hidden_size = 64
#     num_heads = 4
#     num_layers = 2
#     batch_size = 32

#     # Prepare datasets and dataloaders
#     train_dataset = TFTDataset(x_train, y_train, static_train)
#     val_dataset = TFTDataset(x_val, y_val, static_val)
#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
#     val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

#     # Initialize model, optimizer, and loss function
#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
#     model = TemporalFusionTransformer(input_dim, static_dim, output_dim, hidden_size, num_heads, num_layers).to(device)
#     optimizer = optim.Adam(model.parameters(), lr=0.001)
#     criterion = CustomLoss()


#     # Train the model
#     history = train_model(model, train_loader, val_loader, optimizer, criterion, device, num_epochs)

#     # Plot training history
#     plt.plot(history['train_loss'], label='Train Loss')
#     plt.plot(history['val_loss'], label='Validation Loss')
#     plt.xlabel('Epochs')
#     plt.ylabel('Loss')
#     plt.title('Training and Validation Loss')
#     plt.legend()
#     plt.show()

#     # Save the model
#     today = datetime.now().strftime("%d-%m-%Y-%H-%M-%S")
#     model_save_path = os.path.join(model_path, 'TFT_model_search_' + today + '.pt')
#     torch.save(model.state_dict(), model_save_path)
#     print(f"Model saved to {model_save_path}")
#     return model, model_save_path




import os
import pandas as pd
import numpy as np
import torch
from datetime import datetime
from sklearn.metrics import mean_absolute_error, mean_squared_error
from torch.utils.data import DataLoader, Dataset

from sklearn.metrics import mean_absolute_error, mean_squared_error

def root_mean_squared_error(y_true, y_pred):
    return np.sqrt(mean_squared_error(y_true, y_pred))

TRAINING=False
# Dynamically determine input_dim and output_dim based on non-augmented data for consistency
input_dim = x_train[:, :, 2:].shape[2]  # Sliced feature size
output_dim = y_train.shape[1]          # Output size (targets)

# Predefined hyperparameters
hidden_size = 64
num_heads = 4
num_layers = 2  # Increase layers to address LSTM dropout warning
dropout = 0.3 if num_layers > 1 else 0  # Set dropout only if num_layers > 1

# Train the model using augmented data
if TRAINING:
    print("Training the model with augmented data...")
    model, model_path = main_pipeline(
        x_train_a[:, :, 2:], y_train_a,  # Augmented data for training
        x_val_a[:, :, 2:], y_val_a,  # Augmented validation data
        # static_train_a, static_val_a,
        model_path=output_path,
        num_epochs=20,
        training=True
    )
    print(f"Model saved to {model_path}")
else:
    print("Checking for saved models...")
    tft_models = glob(os.path.join(output_path, "TFT_model_search_*.pt"))
    if not tft_models:
        print('No file with such pattern was found in the directory. Run TRAINING = True first.')
        raise FileNotFoundError("Saved model not found. Train the model first.")
    else:
        # Load the latest saved model
        model_path = tft_models[-1]
        print(f"Loading model from: {model_path}")
        model = TemporalFusionTransformer(input_dim, output_dim, hidden_size, num_heads)
        model.load_state_dict(torch.load(model_path))
        model.eval()
    
    trainT, valT = dataset_handler.prepare_data_LSTM(x_train[:, :, 2:], y_train, x_val[:, :, 2:], y_val)

    # Ensure the model dimensions match the data
    assert trainT[0].shape[-1] == input_dim, f"Mismatch: input_dim={input_dim}, trainT features={trainT[0].shape[-1]}"

    # Create index mappings
    y_val_indices_df = pd.DataFrame(val_indices, columns=['actual_index'])
    y_train_indices_df = pd.DataFrame(train_indices, columns=['actual_index'])
    
    # Make predictions on training and validation sets
    with torch.no_grad():
        preds_tra = model(torch.tensor(trainT[0], dtype=torch.float32)).numpy()
        preds_tra[preds_tra < 0] = 0
        preds_val = model(torch.tensor(valT[0], dtype=torch.float32)).numpy()
        preds_val[preds_val < 0] = 0
    
    results = []
    preds_val_original = scaler.inverse_transform(preds_val)
    y_val_original = scaler.inverse_transform(valT[1])
    
    # Inverse transform predictions and ground truth values for training data
    preds_tra_original = scaler.inverse_transform(preds_tra)
    y_train_original = scaler.inverse_transform(trainT[1])
    
    for department_idx, department_name in DEP_NAMES.items():
        # Filter rows corresponding to the current department for both training and validation
        department_rows_val = validation_dataframe[validation_dataframe['dep_id'] == department_idx]
        department_rows_train = training_dataframe[training_dataframe['dep_id'] == department_idx]
    
        if department_rows_val.empty or department_rows_train.empty:
            continue
    
        department_indices_val = department_rows_val.index.tolist()
        department_indices_train = department_rows_train.index.tolist()
    
        # Matching indices for validation data
        matching_indices_val = y_val_indices_df[y_val_indices_df['actual_index'].isin(department_indices_val)].index
        # Matching indices for training data
        matching_indices_train = y_train_indices_df[y_train_indices_df['actual_index'].isin(department_indices_train)].index
    
        if matching_indices_val.empty or matching_indices_train.empty:
            continue
    
        # Extract original scale true and predicted values for validation data
        true_dengrate_all_val = y_val_original[matching_indices_val, 0]
        true_dengrate_019_val = y_val_original[matching_indices_val, 1]
        predicted_dengrate_all_val = preds_val_original[matching_indices_val, 0]
        predicted_dengrate_019_val = preds_val_original[matching_indices_val, 1]
    
        # Extract original scale true and predicted values for training data
        true_dengrate_all_train = y_train_original[matching_indices_train, 0]
        true_dengrate_019_train = y_train_original[matching_indices_train, 1]
        predicted_dengrate_all_train = preds_tra_original[matching_indices_train, 0]
        predicted_dengrate_019_train = preds_tra_original[matching_indices_train, 1]
    
        # Calculate NRMSE for both DengRate_all and DengRate_019 (validation data)
        nrmse_dengrate_all_val = root_mean_squared_error(true_dengrate_all_val, predicted_dengrate_all_val) / (true_dengrate_all_val.max() - true_dengrate_all_val.min())
        nrmse_dengrate_019_val = root_mean_squared_error(true_dengrate_019_val, predicted_dengrate_019_val) / (true_dengrate_019_val.max() - true_dengrate_019_val.min())
    
        # Calculate NRMSE for both DengRate_all and DengRate_019 (training data)
        nrmse_dengrate_all_train = root_mean_squared_error(true_dengrate_all_train, predicted_dengrate_all_train) / (true_dengrate_all_train.max() - true_dengrate_all_train.min())
        nrmse_dengrate_019_train = root_mean_squared_error(true_dengrate_019_train, predicted_dengrate_019_train) / (true_dengrate_019_train.max() - true_dengrate_019_train.min())
    
        # Calculate MAE for both DengRate_all and DengRate_019 (validation data)
        mae_dengrate_all_val = mean_absolute_error(true_dengrate_all_val, predicted_dengrate_all_val)
        mae_dengrate_019_val = mean_absolute_error(true_dengrate_019_val, predicted_dengrate_019_val)
    
        # Store the results
        results.append({
            'Department': department_name,
            'NRMSE 0-19 Training': nrmse_dengrate_019_train,
            'NRMSE All Training': nrmse_dengrate_all_train,
            'NRMSE 0-19 Validation': nrmse_dengrate_019_val,
            'NRMSE All Validation': nrmse_dengrate_all_val,
            'MAE (DengRate_all) Val': mae_dengrate_all_val,
            'MAE (DengRate_019) Val': mae_dengrate_019_val
        })
    
    # Save results
    results_df = pd.DataFrame(results)
    today = datetime.now().strftime("%d-%m-%Y-%H-%M-%S")
    output_file = os.path.join(config['metrics'], "Brazil", f'TFT_new_model_search_{today}.csv')
    
    # Create the directory if it doesn't exist
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    results_df.to_csv(output_file, index=False)
    print(f"Results saved to {output_file}")











